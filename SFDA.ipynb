{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNgfEBIi47IZptxcz5wfKIJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Parv-Agarwal/Internship-project/blob/main/SFDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "VfOx6356wEiN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import os\n",
        "\n",
        "class MNISTDataset(Dataset):\n",
        "    def __init__(self, file_name, max_load=None, transform=None):\n",
        "        # Load the dataset from the given file\n",
        "        self.transform = transform\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Load data\n",
        "        if 'train' in file_name:\n",
        "            dataset = datasets.MNIST(root='./data', train=True, download=True)\n",
        "        else:\n",
        "            dataset = datasets.MNIST(root='./data', train=False, download=True)\n",
        "\n",
        "        self.data = dataset.data\n",
        "        self.labels = dataset.targets\n",
        "\n",
        "        # Limit the number of examples if max_load is specified\n",
        "        if max_load is not None and max_load > 0 and max_load < len(self.data):\n",
        "            self.data = self.data[:max_load]\n",
        "            self.labels = self.labels[:max_load]\n",
        "            print(f'<mnist> loading only {max_load} examples')\n",
        "\n",
        "        print('<mnist> done')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = self.data[index]\n",
        "        label = self.labels[index]\n",
        "\n",
        "        # Convert tensor to PIL Image\n",
        "        img = transforms.ToPILImage()(img)\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        else:\n",
        "            img = transforms.ToTensor()(img)\n",
        "\n",
        "        # Convert label to one-hot encoding\n",
        "        label_one_hot = torch.zeros(10)\n",
        "        label_one_hot[label] = 1.0\n",
        "\n",
        "        return img, label_one_hot\n"
      ],
      "metadata": {
        "id": "Wl6rpI5ivdLu"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset_mnistM.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "from torchvision import transforms\n",
        "\n",
        "class MNISTMDataset(Dataset):\n",
        "    def __init__(self, file_name, max_load=None, transform=None):\n",
        "        # Load the MNIST-M dataset from the given file\n",
        "        # Assuming the dataset is stored in .npy files or a custom format\n",
        "        self.transform = transform\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Load data from file_name\n",
        "        # For this example, we'll assume data is stored in .pt files\n",
        "        # Replace this with the actual data loading code\n",
        "        if not os.path.isfile(file_name):\n",
        "            raise FileNotFoundError(f\"File {file_name} not found.\")\n",
        "\n",
        "        data_dict = torch.load(file_name)\n",
        "\n",
        "        # Check if data_dict is a tuple and convert to dict if needed\n",
        "        if isinstance(data_dict, tuple):\n",
        "            # Assuming the tuple has data and labels in the first two positions\n",
        "            self.data = data_dict[0]\n",
        "            self.labels = data_dict[1]\n",
        "        else:\n",
        "            self.data = data_dict['data']\n",
        "            self.labels = data_dict['labels']\n",
        "\n",
        "        n_example = self.data.size(0)\n",
        "        print(f'nExample {n_example}')\n",
        "\n",
        "        # Limit the number of examples if max_load is specified\n",
        "        if max_load is not None and max_load > 0 and max_load < n_example:\n",
        "            n_example = max_load\n",
        "            print(f'<mnistM> loading only {n_example} examples')\n",
        "            self.data = self.data[:n_example]\n",
        "            self.labels = self.labels[:n_example]\n",
        "\n",
        "        print('<mnistM> done')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = self.data[index]\n",
        "        label = self.labels[index]\n",
        "\n",
        "        if isinstance(img, np.ndarray):\n",
        "            img = Image.fromarray(img.astype(np.uint8))\n",
        "        elif torch.is_tensor(img):\n",
        "            if img.dim() == 3:\n",
        "                # If the image has shape (H, W, C), permute it to (C, H, W)\n",
        "                img = img.permute(2, 0, 1)\n",
        "            elif img.dim() == 2:\n",
        "                # If the image has shape (H, W), add a channel dimension\n",
        "                img = img.unsqueeze(0)\n",
        "            img = transforms.ToPILImage()(img)\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        else:\n",
        "            img = transforms.ToTensor()(img)\n",
        "\n",
        "        # Convert label to one-hot encoding\n",
        "        label_one_hot = torch.zeros(10)\n",
        "        label_one_hot[label] = 1.0\n",
        "\n",
        "        return img, label_one_hot\n"
      ],
      "metadata": {
        "id": "wnS76Ybucp9Q"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogSumExp(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LogSumExp, self).__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        max_val, _ = torch.max(input, dim=1, keepdim=True)\n",
        "        output = input - max_val\n",
        "        output = max_val + torch.log(torch.sum(torch.exp(output), dim=1, keepdim=True))\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "4a_zbndG2WVH"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = {\n",
        "    'dataset': 'mnist',\n",
        "    'batchSize': 64,\n",
        "    'loadSize': 33,\n",
        "    'fineSize': 32,\n",
        "    'nz': 100,               # # of dim for Z\n",
        "    'ngf': 64,               # # of gen filters in first conv layer\n",
        "    'ndf': 64,               # # of discrim filters in first conv layer\n",
        "    'nThreads': 4,           # # of data loading threads to use\n",
        "    'niter': 10000,          # # of iter at starting learning rate\n",
        "    'lr': 0.0002,            # initial learning rate for adam\n",
        "    'beta1': 0.5,            # momentum term of adam\n",
        "    'ntrain': float('inf'),  # # of examples per epoch\n",
        "    'display': 0,            # display samples while training\n",
        "    'display_id': 0,         # display window id\n",
        "    'gpu': 1,                # gpu = 0 is CPU mode. gpu=X is GPU mode on GPU X\n",
        "    'name': 'Logfiles',\n",
        "    'noise': 'normal',       # 'uniform' or 'normal'\n",
        "    'epoch_save_modulo': 1,\n",
        "    'manual_seed': 4,        # Seed\n",
        "    'nc': 3,                 # # of channels in input\n",
        "    'save': 'logs/',         # Directory to save logs\n",
        "    'data_root': './data',   # Root directory for datasets\n",
        "    'lamda': 1,              # Lambda value for GRL\n",
        "    'baseLearningRate': 0.0002,\n",
        "    'max_epoch': 10000,\n",
        "    'gamma': 0.001,\n",
        "    'power': 0.75,\n",
        "    'max_epoch_grl': 10000,\n",
        "    'alpha': 10,\n",
        "}\n",
        "\n",
        "train_gen_epoch = 25"
      ],
      "metadata": {
        "id": "u0-KvzP4yNFt"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed\n",
        "import random\n",
        "random.seed(opt['manual_seed'])\n",
        "torch.manual_seed(opt['manual_seed'])\n",
        "torch.set_num_threads(1)\n",
        "\n",
        "if torch.cuda.is_available() and opt['gpu'] > 0:\n",
        "    torch.cuda.manual_seed_all(opt['manual_seed'])\n",
        "    device = torch.device(f'cuda:{opt[\"gpu\"] - 1}')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "print(f\"Random Seed: {opt['manual_seed']}\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Initialize data loaders\n",
        "transform_mnist = transforms.Compose([\n",
        "    transforms.Resize(opt['fineSize']),\n",
        "    transforms.Grayscale(num_output_channels=3),  # Converts grayscale to 3 channels\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Adjusted for 3 channels\n",
        "])\n",
        "\n",
        "transform_mnistM = transforms.Compose([\n",
        "    transforms.Resize(opt['fineSize']),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRyTQzy4y1x2",
        "outputId": "89cfeba4-b4bd-456f-d70b-f0e40056ecaa"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Seed: 4\n",
            "Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_train_path = 'mnist_train.pt'  # Adjust the path as needed\n",
        "mnist_test_path = 'mnist_test.pt'    # Adjust the path as needed\n",
        "max_train_load = None  # Set to None or an integer value\n",
        "max_test_load = None   # Set to None or an integer value\n",
        "\n",
        "mnist_train_dataset = MNISTDataset(mnist_train_path, max_load=max_train_load, transform=transform_mnist)\n",
        "mnist_test_dataset = MNISTDataset(mnist_test_path, max_load=max_test_load, transform=transform_mnist)\n",
        "\n",
        "mnist_train_loader = DataLoader(mnist_train_dataset, batch_size=opt['batchSize'], shuffle=True, num_workers=opt['nThreads'])\n",
        "mnist_test_loader = DataLoader(mnist_test_dataset, batch_size=opt['batchSize'], shuffle=False, num_workers=opt['nThreads'])\n",
        "\n",
        "print(f\"MNIST Dataset: Size: {len(mnist_train_dataset)}\")\n",
        "\n",
        "# Load MNIST-M dataset\n",
        "mnistm_train_path = 'mnist_m_train.pt'  # Adjust the path as needed\n",
        "mnistm_test_path = 'mnist_m_test.pt'    # Adjust the path as needed\n",
        "Num_Train_Target = 59001\n",
        "Num_Test_Target = 10001\n",
        "\n",
        "mnistm_train_dataset = MNISTMDataset(mnistm_train_path, max_load=Num_Train_Target, transform=transform_mnistM)\n",
        "mnistm_test_dataset = MNISTMDataset(mnistm_test_path, max_load=Num_Test_Target, transform=transform_mnistM)\n",
        "\n",
        "mnistm_train_loader = DataLoader(mnistm_train_dataset, batch_size=opt['batchSize'], shuffle=True, num_workers=opt['nThreads'])\n",
        "mnistm_test_loader = DataLoader(mnistm_test_dataset, batch_size=opt['batchSize'], shuffle=False, num_workers=opt['nThreads'])\n",
        "\n",
        "print(f\"MNIST-M Dataset: Size: {len(mnistm_train_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVoeyuWZzqjP",
        "outputId": "42f2394f-0f6d-4637-b3f1-f46449b08c06"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<mnist> done\n",
            "<mnist> done\n",
            "MNIST Dataset: Size: 60000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-65ace6d6303b>:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  data_dict = torch.load(file_name)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nExample 60000\n",
            "<mnistM> loading only 59001 examples\n",
            "<mnistM> done\n",
            "nExample 10000\n",
            "<mnistM> done\n",
            "MNIST-M Dataset: Size: 59001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize weights\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1 or classname.find('Linear') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias.data, 0)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "metadata": {
        "id": "2_94LGPZ0O0X"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, nz, ngf, nc):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # Input is Z + one-hot class vector\n",
        "            nn.ConvTranspose2d(nz + 10, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # State size: (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # State size: (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # State size: (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d(ngf * 2, nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # Output size: (nc) x 32 x 32\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ],
      "metadata": {
        "id": "OXwP7XO00ViN"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, nc, ndf):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # Input is (nc) x 32 x 32\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),  # (ndf) x 16 x 16\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf, ndf * 4, 4, 2, 1, bias=False),  # (ndf*4) x 8 x 8\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),  # (ndf*8) x 4 x 4\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),  # Output is single value\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input).view(-1, 1)"
      ],
      "metadata": {
        "id": "wn_yjEfW0ZF1"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Function\n",
        "class GradientReversalFunction(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, lambda_):\n",
        "        ctx.lambda_ = lambda_\n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return grad_output.neg() * ctx.lambda_, None\n",
        "\n",
        "def grad_reverse(x, lambda_=1.0):\n",
        "    return GradientReversalFunction.apply(x, lambda_)"
      ],
      "metadata": {
        "id": "_e9P2l1L0fb2"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 5),  # Input channels, output channels, kernel size\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(32, 48, 5),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        self.fc_features = 48 * 5 * 5  # Calculate the output size\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = x.view(-1, self.fc_features)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ic1UnXRD0zZP"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ClassClassifier, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(48 * 5 * 5, 100),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(100, 100),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(100, 10),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "mfc2NkUD1Bn9"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DomainClassifier(nn.Module):\n",
        "    def __init__(self, lambda_=1.0):\n",
        "        super(DomainClassifier, self).__init__()\n",
        "        self.lambda_ = lambda_\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(48 * 5 * 5, 100),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(100, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = grad_reverse(x, self.lambda_)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "zYjEREoU1FE-"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "netG = Generator(opt['nz'], opt['ngf'], opt['nc']).to(device)\n",
        "netD = Discriminator(opt['nc'], opt['ndf']).to(device)\n",
        "feature_extractor = FeatureExtractor().to(device)\n",
        "class_classifier = ClassClassifier().to(device)\n",
        "domain_classifier = DomainClassifier(lambda_=opt['lamda']).to(device)\n",
        "\n",
        "netG.apply(weights_init)\n",
        "netD.apply(weights_init)\n",
        "feature_extractor.apply(weights_init)\n",
        "class_classifier.apply(weights_init)\n",
        "domain_classifier.apply(weights_init)\n",
        "\n",
        "# print model summaries:\n",
        "\n",
        "print(\"Generator Model Summary:\")\n",
        "summary(netG, input_size=(opt['nz'] + 10, 1, 1))\n",
        "\n",
        "print(\"\\nDiscriminator Model Summary:\")\n",
        "summary(netD, input_size=(opt['nc'], opt['fineSize'], opt['fineSize']))\n",
        "\n",
        "print(\"\\nFeature Extractor Model Summary:\")\n",
        "summary(feature_extractor, input_size=(opt['nc'], opt['fineSize'], opt['fineSize']))\n",
        "\n",
        "print(\"\\nClass Classifier Model Summary:\")\n",
        "summary(class_classifier, input_size=(48 * 5 * 5,))\n",
        "\n",
        "print(\"\\nDomain Classifier Model Summary:\")\n",
        "summary(domain_classifier, input_size=(48 * 5 * 5,))\n",
        "\n",
        "\n",
        "\n",
        "# Loss functions\n",
        "adversarial_loss = nn.BCELoss().to(device)\n",
        "classification_loss = nn.NLLLoss().to(device)\n",
        "cross_entropy_loss = nn.CrossEntropyLoss().to(device)\n",
        "log_sum_exp = LogSumExp().to(device)\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = optim.Adam(netG.parameters(), lr=opt['lr'], betas=(opt['beta1'], 0.999))\n",
        "optimizer_D = optim.Adam(netD.parameters(), lr=opt['lr'], betas=(opt['beta1'], 0.999))\n",
        "optimizer_feature = optim.SGD(feature_extractor.parameters(), lr=opt['baseLearningRate'], momentum=0.9)\n",
        "optimizer_class = optim.SGD(class_classifier.parameters(), lr=opt['baseLearningRate'], momentum=0.9)\n",
        "optimizer_domain = optim.SGD(domain_classifier.parameters(), lr=opt['baseLearningRate'], momentum=0.9)"
      ],
      "metadata": {
        "id": "WWJHLYWa1GlW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "503d9971-aa81-4c77-a18a-80d5b105cd01"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Model Summary:\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "   ConvTranspose2d-1            [-1, 512, 4, 4]         901,120\n",
            "       BatchNorm2d-2            [-1, 512, 4, 4]           1,024\n",
            "              ReLU-3            [-1, 512, 4, 4]               0\n",
            "   ConvTranspose2d-4            [-1, 256, 8, 8]       2,097,152\n",
            "       BatchNorm2d-5            [-1, 256, 8, 8]             512\n",
            "              ReLU-6            [-1, 256, 8, 8]               0\n",
            "   ConvTranspose2d-7          [-1, 128, 16, 16]         524,288\n",
            "       BatchNorm2d-8          [-1, 128, 16, 16]             256\n",
            "              ReLU-9          [-1, 128, 16, 16]               0\n",
            "  ConvTranspose2d-10            [-1, 3, 32, 32]           6,144\n",
            "             Tanh-11            [-1, 3, 32, 32]               0\n",
            "================================================================\n",
            "Total params: 3,530,496\n",
            "Trainable params: 3,530,496\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.36\n",
            "Params size (MB): 13.47\n",
            "Estimated Total Size (MB): 14.83\n",
            "----------------------------------------------------------------\n",
            "\n",
            "Discriminator Model Summary:\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 16, 16]           3,072\n",
            "         LeakyReLU-2           [-1, 64, 16, 16]               0\n",
            "            Conv2d-3            [-1, 256, 8, 8]         262,144\n",
            "       BatchNorm2d-4            [-1, 256, 8, 8]             512\n",
            "         LeakyReLU-5            [-1, 256, 8, 8]               0\n",
            "            Conv2d-6            [-1, 512, 4, 4]       2,097,152\n",
            "       BatchNorm2d-7            [-1, 512, 4, 4]           1,024\n",
            "         LeakyReLU-8            [-1, 512, 4, 4]               0\n",
            "            Conv2d-9              [-1, 1, 1, 1]           8,192\n",
            "          Sigmoid-10              [-1, 1, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 2,372,096\n",
            "Trainable params: 2,372,096\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.81\n",
            "Params size (MB): 9.05\n",
            "Estimated Total Size (MB): 9.87\n",
            "----------------------------------------------------------------\n",
            "\n",
            "Feature Extractor Model Summary:\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 28, 28]           2,432\n",
            "              ReLU-2           [-1, 32, 28, 28]               0\n",
            "         MaxPool2d-3           [-1, 32, 14, 14]               0\n",
            "            Conv2d-4           [-1, 48, 10, 10]          38,448\n",
            "              ReLU-5           [-1, 48, 10, 10]               0\n",
            "         MaxPool2d-6             [-1, 48, 5, 5]               0\n",
            "================================================================\n",
            "Total params: 40,880\n",
            "Trainable params: 40,880\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.51\n",
            "Params size (MB): 0.16\n",
            "Estimated Total Size (MB): 0.68\n",
            "----------------------------------------------------------------\n",
            "\n",
            "Class Classifier Model Summary:\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                  [-1, 100]         120,100\n",
            "              ReLU-2                  [-1, 100]               0\n",
            "            Linear-3                  [-1, 100]          10,100\n",
            "              ReLU-4                  [-1, 100]               0\n",
            "            Linear-5                   [-1, 10]           1,010\n",
            "        LogSoftmax-6                   [-1, 10]               0\n",
            "================================================================\n",
            "Total params: 131,210\n",
            "Trainable params: 131,210\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.50\n",
            "Estimated Total Size (MB): 0.51\n",
            "----------------------------------------------------------------\n",
            "\n",
            "Domain Classifier Model Summary:\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                  [-1, 100]         120,100\n",
            "              ReLU-2                  [-1, 100]               0\n",
            "            Linear-3                    [-1, 2]             202\n",
            "================================================================\n",
            "Total params: 120,302\n",
            "Trainable params: 120,302\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.46\n",
            "Estimated Total Size (MB): 0.47\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def train(epoch):\n",
        "    netG.train()\n",
        "    netD.train()\n",
        "    feature_extractor.train()\n",
        "    class_classifier.train()\n",
        "    domain_classifier.train()\n",
        "\n",
        "    avg_loss = 0\n",
        "    avg_acc = 0\n",
        "    count = 0\n",
        "\n",
        "    data_iter = iter(mnistm_train_loader)\n",
        "    len_dataloader = min(len(mnist_train_loader), len(mnistm_train_loader))\n",
        "\n",
        "    for batch_idx in range(len_dataloader):\n",
        "        # Get source data (MNIST)\n",
        "        try:\n",
        "            source_data, source_labels_one_hot = next(iter(mnist_train_loader))\n",
        "        except StopIteration:\n",
        "            data_iter = iter(mnist_train_loader)\n",
        "            source_data, source_labels_one_hot = next(data_iter)\n",
        "\n",
        "        # Get target data (MNIST-M)\n",
        "        try:\n",
        "            target_data, _ = next(iter(mnistm_train_loader))\n",
        "        except StopIteration:\n",
        "            data_iter = iter(mnistm_train_loader)\n",
        "            target_data, _ = next(data_iter)\n",
        "\n",
        "        if source_data.size(0) != opt['batchSize'] or target_data.size(0) != opt['batchSize']:\n",
        "            continue  # Skip incomplete batch\n",
        "\n",
        "        # Move data to device\n",
        "        source_data = source_data.to(device)\n",
        "        source_labels = torch.argmax(source_labels_one_hot, dim=1).to(device)\n",
        "        target_data = target_data.to(device)\n",
        "\n",
        "        batch_size = source_data.size(0)\n",
        "        label_real = torch.full((batch_size, 1), 1.0, device=device)\n",
        "        label_fake = torch.full((batch_size, 1), 0.0, device=device)\n",
        "\n",
        "        # Generate fake images\n",
        "        class_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
        "        one_hot_labels = F.one_hot(class_labels, num_classes=10).float()\n",
        "        one_hot_labels = one_hot_labels.view(batch_size, 10, 1, 1).to(device)\n",
        "\n",
        "        # Concatenate noise and one-hot labels\n",
        "        noise = torch.randn(batch_size, opt['nz'], 1, 1, device=device)\n",
        "        noise_with_labels = torch.cat((noise, one_hot_labels), 1)\n",
        "\n",
        "        # Generate fake images\n",
        "        fake_images = netG(noise_with_labels)\n",
        "\n",
        "        # Train Discriminator\n",
        "        netD.zero_grad()\n",
        "        # Discriminator loss on real data\n",
        "        output_real = netD(source_data)\n",
        "        errD_real = adversarial_loss(output_real, label_real)\n",
        "        # Discriminator loss on fake data\n",
        "        output_fake = netD(fake_images.detach())\n",
        "        errD_fake = adversarial_loss(output_fake, label_fake)\n",
        "        # Total discriminator loss\n",
        "        errD = errD_real + errD_fake\n",
        "        errD.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Train Generator\n",
        "        netG.zero_grad()\n",
        "        output_fake = netD(fake_images)\n",
        "        errG = adversarial_loss(output_fake, label_real)\n",
        "        errG.backward(retain_graph=True)\n",
        "\n",
        "        # Compute classification loss\n",
        "        features = feature_extractor(fake_images)\n",
        "        class_outputs = class_classifier(features)\n",
        "        class_loss = cross_entropy_loss(class_outputs, class_labels)\n",
        "        class_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # Update feature extractor and classifiers\n",
        "        feature_extractor.zero_grad()\n",
        "        class_classifier.zero_grad()\n",
        "        domain_classifier.zero_grad()\n",
        "\n",
        "        # Prepare domain labels\n",
        "        source_domain_labels = torch.zeros(batch_size, dtype=torch.long, device=device)\n",
        "        target_domain_labels = torch.ones(batch_size, dtype=torch.long, device=device)\n",
        "\n",
        "        # Forward pass for domain classification\n",
        "        features_source = feature_extractor(fake_images.detach())\n",
        "        features_target = feature_extractor(target_data)\n",
        "        domain_output_source = domain_classifier(features_source)\n",
        "        domain_output_target = domain_classifier(features_target)\n",
        "        domain_output = torch.cat((domain_output_source, domain_output_target), 0)\n",
        "        domain_labels = torch.cat((source_domain_labels, target_domain_labels), 0)\n",
        "\n",
        "        domain_loss = cross_entropy_loss(domain_output, domain_labels)\n",
        "        domain_loss.backward()\n",
        "        optimizer_feature.step()\n",
        "        optimizer_class.step()\n",
        "        optimizer_domain.step()\n",
        "\n",
        "        # Update average loss and accuracy\n",
        "        avg_loss += errG.item()\n",
        "        _, predicted = torch.max(class_outputs.data, 1)\n",
        "        correct = (predicted == class_labels).sum().item()\n",
        "        avg_acc += correct / batch_size\n",
        "        count += 1\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Epoch [{epoch}/{opt[\"niter\"]}] Batch [{batch_idx}/{len_dataloader}] '\n",
        "                  f'Loss D: {errD.item():.4f}, Loss G: {errG.item():.4f}, '\n",
        "                  f'Class Loss: {class_loss.item():.4f}, Domain Loss: {domain_loss.item():.4f}')\n",
        "\n",
        "    avg_loss /= count\n",
        "    avg_acc /= count\n",
        "    return avg_acc, avg_loss"
      ],
      "metadata": {
        "id": "oXedOFMY2a_G"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(epoch):\n",
        "    netG.eval()\n",
        "    feature_extractor.eval()\n",
        "    class_classifier.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, labels_one_hot) in enumerate(mnistm_test_loader):\n",
        "            if data.size(0) != opt['batchSize']:\n",
        "                continue  # Skip incomplete batch\n",
        "\n",
        "            data = data.to(device)\n",
        "            labels = torch.argmax(labels_one_hot, dim=1).to(device)\n",
        "            features = feature_extractor(data)\n",
        "            outputs = class_classifier(features)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "OH1ZipoK2kmG"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, opt['niter'] + 1):\n",
        "    train_acc, train_loss = train(epoch)\n",
        "    if epoch > train_gen_epoch:\n",
        "        test_acc = test(epoch)\n",
        "        #Save model checkpoints if needed\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kTy5Oys32n6I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}