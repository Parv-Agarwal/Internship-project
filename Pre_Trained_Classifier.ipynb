{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMtsNf2XZ2ihQfNUqFI3tVB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Parv-Agarwal/Internship-project/blob/main/Pre_Trained_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TBIIVi4t76Es"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opt = {\n",
        "    'dataset': 'mnist',\n",
        "    'batchSize': 64,\n",
        "    'loadSize': 33,\n",
        "    'fineSize': 32,\n",
        "    'nz': 100,               # # of dim for Z\n",
        "    'ngf': 64,               # # of gen filters in first conv layer\n",
        "    'ndf': 64,               # # of discrim filters in first conv layer\n",
        "    'nThreads': 4,           # # of data loading threads to use\n",
        "    'niter': 10000,          # # of iter at starting learning rate\n",
        "    'lr': 0.0002,            # initial learning rate for adam\n",
        "    'beta1': 0.5,            # momentum term of adam\n",
        "    'ntrain': float('inf'),  # # of examples per epoch\n",
        "    'display': 0,            # display samples while training\n",
        "    'display_id': 0,         # display window id\n",
        "    'gpu': 1,                # gpu = 0 is CPU mode. gpu=X is GPU mode on GPU X\n",
        "    'name': 'Logfiles',\n",
        "    'noise': 'normal',       # 'uniform' or 'normal'\n",
        "    'epoch_save_modulo': 1,\n",
        "    'manual_seed': 4,        # Seed\n",
        "    'nc': 3,                 # # of channels in input\n",
        "    'save': 'logs/',         # Directory to save logs\n",
        "    'data_root': './data',   # Root directory for datasets\n",
        "    'lamda': 1,              # Lambda value for GRL\n",
        "    'baseLearningRate': 0.0002,\n",
        "    'max_epoch': 10000,\n",
        "    'gamma': 0.001,\n",
        "    'power': 0.75,\n",
        "    'max_epoch_grl': 10000,\n",
        "    'alpha': 10,\n",
        "    'num_classes': 10,\n",
        "    'num_epochs': 10,\n",
        "    'image_size': 32,\n",
        "}\n"
      ],
      "metadata": {
        "id": "qeES5m30GYkh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed\n",
        "import random\n",
        "random.seed(opt['manual_seed'])\n",
        "torch.manual_seed(opt['manual_seed'])\n",
        "torch.set_num_threads(1)\n",
        "\n",
        "if torch.cuda.is_available() and opt['gpu'] > 0:\n",
        "    torch.cuda.manual_seed_all(opt['manual_seed'])\n",
        "    device = torch.device(f'cuda:{opt[\"gpu\"] - 1}')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "print(f\"Random Seed: {opt['manual_seed']}\")\n",
        "print(f\"Device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BzZAASoF_0R",
        "outputId": "5131ba87-83a7-4c52-b1a5-fec23e3702ad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Seed: 4\n",
            "Device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize with MNIST mean and std\n",
        "])\n",
        "\n",
        "# Load MNIST dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tolGCZBPR0b2",
        "outputId": "6a68e5a3-c301-45c8-cafb-23191d2bdce3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:02<00:00, 4656551.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 134888.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:06<00:00, 246252.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 2780287.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PreTrainedClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PreTrainedClassifier, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(28 * 28, 256),  # Input size is 28*28 for MNIST images\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(64, 10),  # 10 output classes for digits 0-9\n",
        "            nn.LogSoftmax(dim=1)  # LogSoftmax for classification\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)  # Flatten the input (batch_size, 28*28)\n",
        "        return self.fc(x)\n"
      ],
      "metadata": {
        "id": "r6atmkyWSFB-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PreTrainedClassifier().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training function\n",
        "def train(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)  # Move data to the device\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(data)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
        "\n",
        "    return running_loss / len(train_loader)\n",
        "\n",
        "# Validation function\n",
        "def validate(model, test_loader, criterion, device):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(data)\n",
        "\n",
        "            # Calculate loss\n",
        "            test_loss += criterion(output, target).item()\n",
        "\n",
        "            # Get the index of the max log-probability\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "    print(f'Average Test Loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)')\n",
        "    return test_loss, accuracy\n"
      ],
      "metadata": {
        "id": "xjekaSPyDEO5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    print(f'Epoch {epoch}/{num_epochs}')\n",
        "\n",
        "    # Train the model\n",
        "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "    # Validate the model\n",
        "    test_loss, accuracy = validate(model, test_loader, criterion, device)\n",
        "\n",
        "    print(f'Epoch {epoch} Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'pre_trained_mnist_classifier.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50oruwJKT12X",
        "outputId": "b87dacce-3197-4f24-ea5f-b7ec669a1b15"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Batch 0/938, Loss: 2.3180\n",
            "Batch 100/938, Loss: 0.3620\n",
            "Batch 200/938, Loss: 0.2353\n",
            "Batch 300/938, Loss: 0.0997\n",
            "Batch 400/938, Loss: 0.1727\n",
            "Batch 500/938, Loss: 0.1763\n",
            "Batch 600/938, Loss: 0.1232\n",
            "Batch 700/938, Loss: 0.1176\n",
            "Batch 800/938, Loss: 0.1509\n",
            "Batch 900/938, Loss: 0.0506\n",
            "Average Test Loss: 0.1145, Accuracy: 9644/10000 (96.44%)\n",
            "Epoch 1 Train Loss: 0.2552 | Test Loss: 0.1145 | Accuracy: 96.44%\n",
            "Epoch 2/10\n",
            "Batch 0/938, Loss: 0.0758\n",
            "Batch 100/938, Loss: 0.1046\n",
            "Batch 200/938, Loss: 0.2735\n",
            "Batch 300/938, Loss: 0.0488\n",
            "Batch 400/938, Loss: 0.0840\n",
            "Batch 500/938, Loss: 0.2048\n",
            "Batch 600/938, Loss: 0.0359\n",
            "Batch 700/938, Loss: 0.1151\n",
            "Batch 800/938, Loss: 0.0594\n",
            "Batch 900/938, Loss: 0.0334\n",
            "Average Test Loss: 0.0972, Accuracy: 9697/10000 (96.97%)\n",
            "Epoch 2 Train Loss: 0.1008 | Test Loss: 0.0972 | Accuracy: 96.97%\n",
            "Epoch 3/10\n",
            "Batch 0/938, Loss: 0.1219\n",
            "Batch 100/938, Loss: 0.0550\n",
            "Batch 200/938, Loss: 0.1024\n",
            "Batch 300/938, Loss: 0.0241\n",
            "Batch 400/938, Loss: 0.0078\n",
            "Batch 500/938, Loss: 0.0252\n",
            "Batch 600/938, Loss: 0.0204\n",
            "Batch 700/938, Loss: 0.0850\n",
            "Batch 800/938, Loss: 0.0563\n",
            "Batch 900/938, Loss: 0.1204\n",
            "Average Test Loss: 0.0916, Accuracy: 9693/10000 (96.93%)\n",
            "Epoch 3 Train Loss: 0.0712 | Test Loss: 0.0916 | Accuracy: 96.93%\n",
            "Epoch 4/10\n",
            "Batch 0/938, Loss: 0.0404\n",
            "Batch 100/938, Loss: 0.0087\n",
            "Batch 200/938, Loss: 0.0305\n",
            "Batch 300/938, Loss: 0.2587\n",
            "Batch 400/938, Loss: 0.1485\n",
            "Batch 500/938, Loss: 0.0143\n",
            "Batch 600/938, Loss: 0.0373\n",
            "Batch 700/938, Loss: 0.1162\n",
            "Batch 800/938, Loss: 0.0251\n",
            "Batch 900/938, Loss: 0.0655\n",
            "Average Test Loss: 0.0762, Accuracy: 9756/10000 (97.56%)\n",
            "Epoch 4 Train Loss: 0.0548 | Test Loss: 0.0762 | Accuracy: 97.56%\n",
            "Epoch 5/10\n",
            "Batch 0/938, Loss: 0.0533\n",
            "Batch 100/938, Loss: 0.0046\n",
            "Batch 200/938, Loss: 0.0791\n",
            "Batch 300/938, Loss: 0.0446\n",
            "Batch 400/938, Loss: 0.0207\n",
            "Batch 500/938, Loss: 0.0669\n",
            "Batch 600/938, Loss: 0.0309\n",
            "Batch 700/938, Loss: 0.0297\n",
            "Batch 800/938, Loss: 0.0762\n",
            "Batch 900/938, Loss: 0.0024\n",
            "Average Test Loss: 0.0776, Accuracy: 9774/10000 (97.74%)\n",
            "Epoch 5 Train Loss: 0.0446 | Test Loss: 0.0776 | Accuracy: 97.74%\n",
            "Epoch 6/10\n",
            "Batch 0/938, Loss: 0.0112\n",
            "Batch 100/938, Loss: 0.0008\n",
            "Batch 200/938, Loss: 0.0862\n",
            "Batch 300/938, Loss: 0.0909\n",
            "Batch 400/938, Loss: 0.0979\n",
            "Batch 500/938, Loss: 0.0065\n",
            "Batch 600/938, Loss: 0.1098\n",
            "Batch 700/938, Loss: 0.1598\n",
            "Batch 800/938, Loss: 0.0045\n",
            "Batch 900/938, Loss: 0.1680\n",
            "Average Test Loss: 0.0912, Accuracy: 9752/10000 (97.52%)\n",
            "Epoch 6 Train Loss: 0.0374 | Test Loss: 0.0912 | Accuracy: 97.52%\n",
            "Epoch 7/10\n",
            "Batch 0/938, Loss: 0.0137\n",
            "Batch 100/938, Loss: 0.0135\n",
            "Batch 200/938, Loss: 0.0014\n",
            "Batch 300/938, Loss: 0.0126\n",
            "Batch 400/938, Loss: 0.0710\n",
            "Batch 500/938, Loss: 0.0904\n",
            "Batch 600/938, Loss: 0.0351\n",
            "Batch 700/938, Loss: 0.0220\n",
            "Batch 800/938, Loss: 0.0036\n",
            "Batch 900/938, Loss: 0.0301\n",
            "Average Test Loss: 0.0951, Accuracy: 9756/10000 (97.56%)\n",
            "Epoch 7 Train Loss: 0.0325 | Test Loss: 0.0951 | Accuracy: 97.56%\n",
            "Epoch 8/10\n",
            "Batch 0/938, Loss: 0.0469\n",
            "Batch 100/938, Loss: 0.0021\n",
            "Batch 200/938, Loss: 0.0995\n",
            "Batch 300/938, Loss: 0.0461\n",
            "Batch 400/938, Loss: 0.0013\n",
            "Batch 500/938, Loss: 0.0932\n",
            "Batch 600/938, Loss: 0.0028\n",
            "Batch 700/938, Loss: 0.0514\n",
            "Batch 800/938, Loss: 0.0015\n",
            "Batch 900/938, Loss: 0.0139\n",
            "Average Test Loss: 0.0972, Accuracy: 9764/10000 (97.64%)\n",
            "Epoch 8 Train Loss: 0.0267 | Test Loss: 0.0972 | Accuracy: 97.64%\n",
            "Epoch 9/10\n",
            "Batch 0/938, Loss: 0.0476\n",
            "Batch 100/938, Loss: 0.0271\n",
            "Batch 200/938, Loss: 0.0038\n",
            "Batch 300/938, Loss: 0.0224\n",
            "Batch 400/938, Loss: 0.0036\n",
            "Batch 500/938, Loss: 0.0011\n",
            "Batch 600/938, Loss: 0.2429\n",
            "Batch 700/938, Loss: 0.0936\n",
            "Batch 800/938, Loss: 0.0202\n",
            "Batch 900/938, Loss: 0.0490\n",
            "Average Test Loss: 0.0899, Accuracy: 9786/10000 (97.86%)\n",
            "Epoch 9 Train Loss: 0.0254 | Test Loss: 0.0899 | Accuracy: 97.86%\n",
            "Epoch 10/10\n",
            "Batch 0/938, Loss: 0.0026\n",
            "Batch 100/938, Loss: 0.0022\n",
            "Batch 200/938, Loss: 0.0005\n",
            "Batch 300/938, Loss: 0.0016\n",
            "Batch 400/938, Loss: 0.0004\n",
            "Batch 500/938, Loss: 0.0193\n",
            "Batch 600/938, Loss: 0.0096\n",
            "Batch 700/938, Loss: 0.0034\n",
            "Batch 800/938, Loss: 0.0164\n",
            "Batch 900/938, Loss: 0.0258\n",
            "Average Test Loss: 0.0833, Accuracy: 9792/10000 (97.92%)\n",
            "Epoch 10 Train Loss: 0.0221 | Test Loss: 0.0833 | Accuracy: 97.92%\n"
          ]
        }
      ]
    }
  ]
}