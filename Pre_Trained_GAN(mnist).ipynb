{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Parv-Agarwal/Internship-project/blob/main/Pre_Trained_GAN(mnist).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i2tcTT8b7xSl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import os\n",
        "\n",
        "class MNISTDataset(Dataset):\n",
        "    def __init__(self, file_name, max_load=None, transform=None):\n",
        "        # Load the dataset from the given file\n",
        "        self.transform = transform\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Load data\n",
        "        if 'train' in file_name:\n",
        "            dataset = datasets.MNIST(root='./data', train=True, download=True)\n",
        "        else:\n",
        "            dataset = datasets.MNIST(root='./data', train=False, download=True)\n",
        "\n",
        "        self.data = dataset.data\n",
        "        self.labels = dataset.targets\n",
        "\n",
        "        # Limit the number of examples if max_load is specified\n",
        "        if max_load is not None and max_load > 0 and max_load < len(self.data):\n",
        "            self.data = self.data[:max_load]\n",
        "            self.labels = self.labels[:max_load]\n",
        "            print(f'<mnist> loading only {max_load} examples')\n",
        "\n",
        "        print('<mnist> done')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = self.data[index]\n",
        "        label = self.labels[index]\n",
        "\n",
        "        # Convert tensor to PIL Image\n",
        "        img = transforms.ToPILImage()(img)\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        else:\n",
        "            img = transforms.ToTensor()(img)\n",
        "\n",
        "        # Return label as an integer\n",
        "        return img, label\n"
      ],
      "metadata": {
        "id": "mp9U8tIaB_GK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = {\n",
        "    'dataset': 'mnist',\n",
        "    'batchSize': 64,\n",
        "    'loadSize': 33,\n",
        "    'fineSize': 32,\n",
        "    'nz': 100,               # # of dim for Z\n",
        "    'ngf': 64,               # # of gen filters in first conv layer\n",
        "    'ndf': 64,               # # of discrim filters in first conv layer\n",
        "    'nThreads': 4,           # # of data loading threads to use\n",
        "    'niter': 10000,          # # of iter at starting learning rate\n",
        "    'lr': 0.0002,            # initial learning rate for adam\n",
        "    'beta1': 0.5,            # momentum term of adam\n",
        "    'ntrain': float('inf'),  # # of examples per epoch\n",
        "    'display': 0,            # display samples while training\n",
        "    'display_id': 0,         # display window id\n",
        "    'gpu': 1,                # gpu = 0 is CPU mode. gpu=X is GPU mode on GPU X\n",
        "    'name': 'Logfiles',\n",
        "    'noise': 'normal',       # 'uniform' or 'normal'\n",
        "    'epoch_save_modulo': 1,\n",
        "    'manual_seed': 4,        # Seed\n",
        "    'nc': 3,                 # # of channels in input\n",
        "    'save': 'logs/',         # Directory to save logs\n",
        "    'data_root': './data',   # Root directory for datasets\n",
        "    'lamda': 1,              # Lambda value for GRL\n",
        "    'baseLearningRate': 0.0002,\n",
        "    'max_epoch': 10000,\n",
        "    'gamma': 0.001,\n",
        "    'power': 0.75,\n",
        "    'max_epoch_grl': 10000,\n",
        "    'alpha': 10,\n",
        "    'num_classes': 10,\n",
        "    'num_epochs': 50,\n",
        "    'image_size': 32,\n",
        "}\n",
        "\n",
        "train_gen_epoch = 25"
      ],
      "metadata": {
        "id": "q4sjfb5VCINK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed\n",
        "import random\n",
        "random.seed(opt['manual_seed'])\n",
        "torch.manual_seed(opt['manual_seed'])\n",
        "torch.set_num_threads(1)\n",
        "\n",
        "if torch.cuda.is_available() and opt['gpu'] > 0:\n",
        "    torch.cuda.manual_seed_all(opt['manual_seed'])\n",
        "    device = torch.device(f'cuda:{opt[\"gpu\"] - 1}')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "print(f\"Random Seed: {opt['manual_seed']}\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Initialize data loaders\n",
        "transform_mnist = transforms.Compose([\n",
        "    transforms.Resize(opt['fineSize']),\n",
        "    transforms.Grayscale(num_output_channels=3),  # Converts grayscale to 3 channels\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Adjusted for 3 channels\n",
        "])\n"
      ],
      "metadata": {
        "id": "UQlAKz08CL3B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e8a02e1-97d6-4d3d-e3da-7c470a699302"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Seed: 4\n",
            "Device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_train_path = 'mnist_train.pt'  # Adjust the path as needed\n",
        "mnist_test_path = 'mnist_test.pt'    # Adjust the path as needed\n",
        "max_train_load = None  # Set to None or an integer value\n",
        "max_test_load = None   # Set to None or an integer value\n",
        "\n",
        "mnist_train_dataset = MNISTDataset(mnist_train_path, max_load=max_train_load, transform=transform_mnist)\n",
        "mnist_test_dataset = MNISTDataset(mnist_test_path, max_load=max_test_load, transform=transform_mnist)\n",
        "\n",
        "mnist_train_loader = DataLoader(mnist_train_dataset, batch_size=opt['batchSize'], shuffle=True, num_workers=opt['nThreads'])\n",
        "mnist_test_loader = DataLoader(mnist_test_dataset, batch_size=opt['batchSize'], shuffle=False, num_workers=opt['nThreads'])\n",
        "\n",
        "print(f\"MNIST Dataset: Size: {len(mnist_train_dataset)}\")"
      ],
      "metadata": {
        "id": "waaBdGiGCZPR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e0bdd00-431c-45d5-cb42-9e50397f86b8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 5.12MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 149kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.41MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 2.91MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "<mnist> done\n",
            "<mnist> done\n",
            "MNIST Dataset: Size: 60000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize weights\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1 or classname.find('Linear') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias.data, 0)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "metadata": {
        "id": "eo6erNQSCaap"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, nz, ngf, nc):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # Input is Z + one-hot class vector\n",
        "            nn.ConvTranspose2d(nz + 10, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # State size: (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # State size: (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # State size: (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d(ngf * 2, nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # Output size: (nc) x 32 x 32\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ],
      "metadata": {
        "id": "ARv3fhj6Cc6w"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, nc, ndf, num_classes):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # Input is (nc + num_classes) x 32 x 32\n",
        "            nn.Conv2d(nc + num_classes, ndf, 4, 2, 1, bias=False),  # Output: (ndf) x 16 x 16\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # Continue with the rest of your layers as before\n",
        "            nn.Conv2d(ndf, ndf * 4, 4, 2, 1, bias=False),  # Output: (ndf*4) x 8 x 8\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),  # Output: (ndf*8) x 4 x 4\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),  # Output: 1 x 1 x 1\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input).view(-1, 1)\n"
      ],
      "metadata": {
        "id": "PrEfh-hsCfex"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import ConcatDataset\n",
        "\n",
        "\n",
        "dataloader = mnist_train_loader\n",
        "netG = Generator(opt['nz'], opt['ngf'], opt['nc']).to(device)\n",
        "netD = Discriminator(opt['nc'], opt['ndf'], opt['num_classes']).to(device)\n",
        "netG.apply(weights_init)\n",
        "netD.apply(weights_init)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Optimizers\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=opt['lr'], betas=(opt['beta1'], 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=opt['lr'], betas=(opt['beta1'], 0.999))\n",
        "\n"
      ],
      "metadata": {
        "id": "Qj9MR-OQCi04"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(opt['num_epochs']):\n",
        "    for i, (real_images, labels) in enumerate(dataloader):\n",
        "        ############################\n",
        "        # (1) Update D network\n",
        "        ###########################\n",
        "        netD.zero_grad()\n",
        "        # Format batch\n",
        "        real_images = real_images.to(device)\n",
        "        labels = labels.long().to(device)\n",
        "        batch_size = real_images.size(0)\n",
        "        real_labels = torch.full((batch_size,), 1., device=device)\n",
        "        fake_labels = torch.full((batch_size,), 0., device=device)\n",
        "\n",
        "        # Create one-hot encoding of labels\n",
        "        labels_one_hot = torch.nn.functional.one_hot(labels, opt['num_classes']).type(torch.float).to(device)\n",
        "        labels_one_hot = labels_one_hot.unsqueeze(2).unsqueeze(3)\n",
        "        labels_one_hot = labels_one_hot.expand(batch_size, opt['num_classes'], opt['image_size'], opt['image_size'])\n",
        "\n",
        "        # Concatenate labels to images\n",
        "        real_images_with_labels = torch.cat((real_images, labels_one_hot), 1)\n",
        "\n",
        "        # Forward pass real batch through D\n",
        "        output = netD(real_images_with_labels).view(-1)\n",
        "        errD_real = criterion(output, real_labels)\n",
        "        errD_real.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        # Generate fake images\n",
        "        noise = torch.randn(batch_size, opt['nz'], 1, 1, device=device)\n",
        "        # Generate random labels\n",
        "        random_labels = torch.randint(0, opt['num_classes'], (batch_size,), device=device)\n",
        "        random_labels_one_hot = torch.nn.functional.one_hot(random_labels, opt['num_classes']).type(torch.float).to(device)\n",
        "        random_labels_one_hot_gen = random_labels_one_hot.unsqueeze(2).unsqueeze(3)\n",
        "        # Concatenate noise and labels for generator\n",
        "        noise_with_labels = torch.cat((noise, random_labels_one_hot_gen), 1)\n",
        "\n",
        "        fake_images = netG(noise_with_labels)\n",
        "\n",
        "        # Concatenate labels to fake images for discriminator\n",
        "        random_labels_one_hot_disc = random_labels_one_hot.unsqueeze(2).unsqueeze(3)\n",
        "        random_labels_one_hot_disc = random_labels_one_hot_disc.expand(batch_size, opt['num_classes'], opt['image_size'], opt['image_size'])\n",
        "        fake_images_with_labels = torch.cat((fake_images.detach(), random_labels_one_hot_disc), 1)\n",
        "\n",
        "        # Classify fake images with D\n",
        "        output = netD(fake_images_with_labels).view(-1)\n",
        "        errD_fake = criterion(output, fake_labels)\n",
        "        errD_fake.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "        errD = errD_real + errD_fake\n",
        "        optimizerD.step()\n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network\n",
        "        ###########################\n",
        "        netG.zero_grad()\n",
        "        # We want D to think the fake images are real\n",
        "        fake_images_with_labels = torch.cat((fake_images, random_labels_one_hot_disc), 1)\n",
        "        output = netD(fake_images_with_labels).view(-1)\n",
        "        errG = criterion(output, real_labels)\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        optimizerG.step()\n",
        "\n",
        "        # Output training stats\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{opt['num_epochs']}] Batch [{i}/{len(dataloader)}] \"\n",
        "                  f'Loss_D: {errD.item():.4f} Loss_G: {errG.item():.4f} '\n",
        "                  f'D(x): {D_x:.4f} D(G(z)): {D_G_z1:.4f}/{D_G_z2:.4f}')\n",
        "\n",
        "# Save the generator model\n",
        "torch.save(netG.state_dict(), 'pre_trained_gan_weights.pth')\n",
        "\n",
        "print(\"Training Finished.\")"
      ],
      "metadata": {
        "id": "R41S743_Cprh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a34e94a1-92b9-4d14-ce21-fc3a6d02f72a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50] Batch [0/938] Loss_D: 1.3405 Loss_G: 1.7016 D(x): 0.4771 D(G(z)): 0.3820/0.2469\n",
            "Epoch [1/50] Batch [100/938] Loss_D: 0.4485 Loss_G: 8.8204 D(x): 0.9670 D(G(z)): 0.2674/0.0007\n",
            "Epoch [1/50] Batch [200/938] Loss_D: 0.4997 Loss_G: 4.1284 D(x): 0.7248 D(G(z)): 0.0328/0.0425\n",
            "Epoch [1/50] Batch [300/938] Loss_D: 1.4247 Loss_G: 0.5309 D(x): 0.3470 D(G(z)): 0.1025/0.6660\n",
            "Epoch [1/50] Batch [400/938] Loss_D: 0.6878 Loss_G: 2.3018 D(x): 0.7711 D(G(z)): 0.2740/0.1321\n",
            "Epoch [1/50] Batch [500/938] Loss_D: 0.5240 Loss_G: 3.5691 D(x): 0.8408 D(G(z)): 0.2452/0.0468\n",
            "Epoch [1/50] Batch [600/938] Loss_D: 0.9528 Loss_G: 2.4277 D(x): 0.7141 D(G(z)): 0.3082/0.1361\n",
            "Epoch [1/50] Batch [700/938] Loss_D: 0.4787 Loss_G: 2.7291 D(x): 0.9052 D(G(z)): 0.2747/0.0931\n",
            "Epoch [1/50] Batch [800/938] Loss_D: 0.7022 Loss_G: 2.2897 D(x): 0.6027 D(G(z)): 0.0755/0.1618\n",
            "Epoch [1/50] Batch [900/938] Loss_D: 0.8857 Loss_G: 2.7814 D(x): 0.8953 D(G(z)): 0.4553/0.1009\n",
            "Epoch [2/50] Batch [0/938] Loss_D: 1.4814 Loss_G: 3.8273 D(x): 0.8840 D(G(z)): 0.6433/0.0394\n",
            "Epoch [2/50] Batch [100/938] Loss_D: 1.3827 Loss_G: 2.5354 D(x): 0.8654 D(G(z)): 0.6346/0.1326\n",
            "Epoch [2/50] Batch [200/938] Loss_D: 1.1742 Loss_G: 1.8695 D(x): 0.4515 D(G(z)): 0.1359/0.2420\n",
            "Epoch [2/50] Batch [300/938] Loss_D: 0.9453 Loss_G: 1.1237 D(x): 0.5664 D(G(z)): 0.2448/0.3692\n",
            "Epoch [2/50] Batch [400/938] Loss_D: 0.8713 Loss_G: 0.6844 D(x): 0.5295 D(G(z)): 0.1236/0.5420\n",
            "Epoch [2/50] Batch [500/938] Loss_D: 1.6379 Loss_G: 4.9105 D(x): 0.9296 D(G(z)): 0.7149/0.0155\n",
            "Epoch [2/50] Batch [600/938] Loss_D: 1.2547 Loss_G: 1.1720 D(x): 0.4216 D(G(z)): 0.2087/0.3808\n",
            "Epoch [2/50] Batch [700/938] Loss_D: 1.3324 Loss_G: 1.3987 D(x): 0.4414 D(G(z)): 0.2435/0.2956\n",
            "Epoch [2/50] Batch [800/938] Loss_D: 1.0589 Loss_G: 2.7875 D(x): 0.8012 D(G(z)): 0.5213/0.0728\n",
            "Epoch [2/50] Batch [900/938] Loss_D: 0.7954 Loss_G: 1.0866 D(x): 0.5721 D(G(z)): 0.1441/0.3832\n",
            "Epoch [3/50] Batch [0/938] Loss_D: 1.1276 Loss_G: 1.0412 D(x): 0.6179 D(G(z)): 0.3620/0.3978\n",
            "Epoch [3/50] Batch [100/938] Loss_D: 0.8912 Loss_G: 1.1231 D(x): 0.5672 D(G(z)): 0.1975/0.3631\n",
            "Epoch [3/50] Batch [200/938] Loss_D: 1.1420 Loss_G: 0.8788 D(x): 0.4245 D(G(z)): 0.1242/0.4641\n",
            "Epoch [3/50] Batch [300/938] Loss_D: 0.9190 Loss_G: 1.8991 D(x): 0.6714 D(G(z)): 0.3593/0.1908\n",
            "Epoch [3/50] Batch [400/938] Loss_D: 0.5201 Loss_G: 1.8372 D(x): 0.7253 D(G(z)): 0.1454/0.1941\n",
            "Epoch [3/50] Batch [500/938] Loss_D: 0.8097 Loss_G: 2.5937 D(x): 0.9082 D(G(z)): 0.4492/0.0952\n",
            "Epoch [3/50] Batch [600/938] Loss_D: 0.5219 Loss_G: 1.6668 D(x): 0.8074 D(G(z)): 0.2309/0.2208\n",
            "Epoch [3/50] Batch [700/938] Loss_D: 1.1020 Loss_G: 0.6820 D(x): 0.4304 D(G(z)): 0.0832/0.5569\n",
            "Epoch [3/50] Batch [800/938] Loss_D: 0.7496 Loss_G: 2.0048 D(x): 0.5831 D(G(z)): 0.1075/0.1915\n",
            "Epoch [3/50] Batch [900/938] Loss_D: 0.3419 Loss_G: 2.5284 D(x): 0.8028 D(G(z)): 0.0917/0.0980\n",
            "Epoch [4/50] Batch [0/938] Loss_D: 0.9243 Loss_G: 0.9106 D(x): 0.4764 D(G(z)): 0.0311/0.4861\n",
            "Epoch [4/50] Batch [100/938] Loss_D: 1.1760 Loss_G: 0.5542 D(x): 0.4169 D(G(z)): 0.0956/0.6389\n",
            "Epoch [4/50] Batch [200/938] Loss_D: 0.2958 Loss_G: 2.7572 D(x): 0.8312 D(G(z)): 0.0843/0.0830\n",
            "Epoch [4/50] Batch [300/938] Loss_D: 0.1782 Loss_G: 4.3605 D(x): 0.9803 D(G(z)): 0.1336/0.0175\n",
            "Epoch [4/50] Batch [400/938] Loss_D: 0.1007 Loss_G: 4.2178 D(x): 0.9689 D(G(z)): 0.0637/0.0212\n",
            "Epoch [4/50] Batch [500/938] Loss_D: 0.1818 Loss_G: 3.5482 D(x): 0.9201 D(G(z)): 0.0842/0.0381\n",
            "Epoch [4/50] Batch [600/938] Loss_D: 0.0662 Loss_G: 4.1568 D(x): 0.9621 D(G(z)): 0.0263/0.0217\n",
            "Epoch [4/50] Batch [700/938] Loss_D: 0.0448 Loss_G: 3.9448 D(x): 0.9783 D(G(z)): 0.0223/0.0257\n",
            "Epoch [4/50] Batch [800/938] Loss_D: 0.7343 Loss_G: 2.5219 D(x): 0.7239 D(G(z)): 0.2281/0.1233\n",
            "Epoch [4/50] Batch [900/938] Loss_D: 0.0536 Loss_G: 4.2798 D(x): 0.9653 D(G(z)): 0.0173/0.0213\n",
            "Epoch [5/50] Batch [0/938] Loss_D: 0.0408 Loss_G: 4.5977 D(x): 0.9704 D(G(z)): 0.0100/0.0129\n",
            "Epoch [5/50] Batch [100/938] Loss_D: 0.0319 Loss_G: 4.7916 D(x): 0.9873 D(G(z)): 0.0186/0.0118\n",
            "Epoch [5/50] Batch [200/938] Loss_D: 0.0266 Loss_G: 5.0133 D(x): 0.9907 D(G(z)): 0.0169/0.0099\n",
            "Epoch [5/50] Batch [300/938] Loss_D: 0.0350 Loss_G: 4.5305 D(x): 0.9807 D(G(z)): 0.0149/0.0158\n",
            "Epoch [5/50] Batch [400/938] Loss_D: 0.8765 Loss_G: 1.4684 D(x): 0.5748 D(G(z)): 0.1876/0.3128\n",
            "Epoch [5/50] Batch [500/938] Loss_D: 0.2317 Loss_G: 4.3733 D(x): 0.9437 D(G(z)): 0.1358/0.0203\n",
            "Epoch [5/50] Batch [600/938] Loss_D: 1.2128 Loss_G: 5.5567 D(x): 0.9559 D(G(z)): 0.6023/0.0115\n",
            "Epoch [5/50] Batch [700/938] Loss_D: 0.0534 Loss_G: 4.3054 D(x): 0.9659 D(G(z)): 0.0177/0.0169\n",
            "Epoch [5/50] Batch [800/938] Loss_D: 0.0972 Loss_G: 4.1368 D(x): 0.9626 D(G(z)): 0.0539/0.0219\n",
            "Epoch [5/50] Batch [900/938] Loss_D: 0.0521 Loss_G: 5.9574 D(x): 0.9541 D(G(z)): 0.0040/0.0042\n",
            "Epoch [6/50] Batch [0/938] Loss_D: 0.0173 Loss_G: 5.5511 D(x): 0.9879 D(G(z)): 0.0050/0.0061\n",
            "Epoch [6/50] Batch [100/938] Loss_D: 0.0297 Loss_G: 4.9423 D(x): 0.9819 D(G(z)): 0.0109/0.0104\n",
            "Epoch [6/50] Batch [200/938] Loss_D: 0.0207 Loss_G: 6.3458 D(x): 0.9821 D(G(z)): 0.0025/0.0025\n",
            "Epoch [6/50] Batch [300/938] Loss_D: 0.0149 Loss_G: 5.9742 D(x): 0.9908 D(G(z)): 0.0056/0.0039\n",
            "Epoch [6/50] Batch [400/938] Loss_D: 1.1798 Loss_G: 1.0598 D(x): 0.4233 D(G(z)): 0.0776/0.4611\n",
            "Epoch [6/50] Batch [500/938] Loss_D: 1.7109 Loss_G: 5.0612 D(x): 0.9844 D(G(z)): 0.7087/0.0183\n",
            "Epoch [6/50] Batch [600/938] Loss_D: 1.9930 Loss_G: 0.3791 D(x): 0.2146 D(G(z)): 0.0070/0.7256\n",
            "Epoch [6/50] Batch [700/938] Loss_D: 0.5694 Loss_G: 4.4953 D(x): 0.9656 D(G(z)): 0.3647/0.0164\n",
            "Epoch [6/50] Batch [800/938] Loss_D: 0.0515 Loss_G: 4.4947 D(x): 0.9643 D(G(z)): 0.0131/0.0178\n",
            "Epoch [6/50] Batch [900/938] Loss_D: 0.0296 Loss_G: 5.2060 D(x): 0.9803 D(G(z)): 0.0088/0.0084\n",
            "Epoch [7/50] Batch [0/938] Loss_D: 0.0275 Loss_G: 4.8672 D(x): 0.9945 D(G(z)): 0.0215/0.0106\n",
            "Epoch [7/50] Batch [100/938] Loss_D: 0.0139 Loss_G: 6.4050 D(x): 0.9884 D(G(z)): 0.0022/0.0027\n",
            "Epoch [7/50] Batch [200/938] Loss_D: 0.8773 Loss_G: 1.9033 D(x): 0.6450 D(G(z)): 0.2642/0.2085\n",
            "Epoch [7/50] Batch [300/938] Loss_D: 0.0919 Loss_G: 4.4866 D(x): 0.9275 D(G(z)): 0.0127/0.0202\n",
            "Epoch [7/50] Batch [400/938] Loss_D: 0.0197 Loss_G: 5.1791 D(x): 0.9874 D(G(z)): 0.0069/0.0083\n",
            "Epoch [7/50] Batch [500/938] Loss_D: 0.5496 Loss_G: 2.1894 D(x): 0.7965 D(G(z)): 0.2305/0.1375\n",
            "Epoch [7/50] Batch [600/938] Loss_D: 0.0688 Loss_G: 4.5104 D(x): 0.9716 D(G(z)): 0.0363/0.0166\n",
            "Epoch [7/50] Batch [700/938] Loss_D: 2.3555 Loss_G: 3.1028 D(x): 0.8631 D(G(z)): 0.7685/0.0775\n",
            "Epoch [7/50] Batch [800/938] Loss_D: 0.0468 Loss_G: 4.6394 D(x): 0.9774 D(G(z)): 0.0231/0.0149\n",
            "Epoch [7/50] Batch [900/938] Loss_D: 0.0069 Loss_G: 5.9582 D(x): 0.9968 D(G(z)): 0.0037/0.0038\n",
            "Epoch [8/50] Batch [0/938] Loss_D: 0.0232 Loss_G: 5.3131 D(x): 0.9894 D(G(z)): 0.0124/0.0067\n",
            "Epoch [8/50] Batch [100/938] Loss_D: 0.0271 Loss_G: 4.9337 D(x): 0.9833 D(G(z)): 0.0099/0.0101\n",
            "Epoch [8/50] Batch [200/938] Loss_D: 0.0115 Loss_G: 5.6655 D(x): 0.9958 D(G(z)): 0.0072/0.0055\n",
            "Epoch [8/50] Batch [300/938] Loss_D: 0.0093 Loss_G: 5.7000 D(x): 0.9967 D(G(z)): 0.0059/0.0048\n",
            "Epoch [8/50] Batch [400/938] Loss_D: 0.6197 Loss_G: 2.7973 D(x): 0.8257 D(G(z)): 0.2923/0.0846\n",
            "Epoch [8/50] Batch [500/938] Loss_D: 0.5722 Loss_G: 3.8473 D(x): 0.9147 D(G(z)): 0.3176/0.0326\n",
            "Epoch [8/50] Batch [600/938] Loss_D: 0.6286 Loss_G: 4.3158 D(x): 0.9796 D(G(z)): 0.3782/0.0237\n",
            "Epoch [8/50] Batch [700/938] Loss_D: 0.1031 Loss_G: 4.4305 D(x): 0.9896 D(G(z)): 0.0832/0.0173\n",
            "Epoch [8/50] Batch [800/938] Loss_D: 0.0644 Loss_G: 4.2555 D(x): 0.9720 D(G(z)): 0.0341/0.0225\n",
            "Epoch [8/50] Batch [900/938] Loss_D: 0.0366 Loss_G: 5.5843 D(x): 0.9680 D(G(z)): 0.0028/0.0064\n",
            "Epoch [9/50] Batch [0/938] Loss_D: 0.0133 Loss_G: 5.1545 D(x): 0.9968 D(G(z)): 0.0100/0.0088\n",
            "Epoch [9/50] Batch [100/938] Loss_D: 0.0137 Loss_G: 5.6152 D(x): 0.9938 D(G(z)): 0.0074/0.0057\n",
            "Epoch [9/50] Batch [200/938] Loss_D: 0.0162 Loss_G: 5.7648 D(x): 0.9930 D(G(z)): 0.0091/0.0051\n",
            "Epoch [9/50] Batch [300/938] Loss_D: 0.0224 Loss_G: 5.5585 D(x): 0.9842 D(G(z)): 0.0050/0.0057\n",
            "Epoch [9/50] Batch [400/938] Loss_D: 0.0061 Loss_G: 6.6878 D(x): 0.9968 D(G(z)): 0.0028/0.0022\n",
            "Epoch [9/50] Batch [500/938] Loss_D: 0.0090 Loss_G: 6.4755 D(x): 0.9931 D(G(z)): 0.0021/0.0025\n",
            "Epoch [9/50] Batch [600/938] Loss_D: 0.8131 Loss_G: 2.0489 D(x): 0.7171 D(G(z)): 0.3017/0.1914\n",
            "Epoch [9/50] Batch [700/938] Loss_D: 0.5181 Loss_G: 3.8536 D(x): 0.9324 D(G(z)): 0.2858/0.0417\n",
            "Epoch [9/50] Batch [800/938] Loss_D: 0.0770 Loss_G: 5.9075 D(x): 0.9923 D(G(z)): 0.0591/0.0045\n",
            "Epoch [9/50] Batch [900/938] Loss_D: 0.2543 Loss_G: 4.1043 D(x): 0.8271 D(G(z)): 0.0316/0.0436\n",
            "Epoch [10/50] Batch [0/938] Loss_D: 0.0822 Loss_G: 4.3075 D(x): 0.9710 D(G(z)): 0.0487/0.0263\n",
            "Epoch [10/50] Batch [100/938] Loss_D: 0.0132 Loss_G: 5.5746 D(x): 0.9950 D(G(z)): 0.0080/0.0063\n",
            "Epoch [10/50] Batch [200/938] Loss_D: 0.4114 Loss_G: 2.8384 D(x): 0.8634 D(G(z)): 0.1865/0.0947\n",
            "Epoch [10/50] Batch [300/938] Loss_D: 0.2919 Loss_G: 3.4384 D(x): 0.8467 D(G(z)): 0.0823/0.0624\n",
            "Epoch [10/50] Batch [400/938] Loss_D: 0.0244 Loss_G: 5.1182 D(x): 0.9886 D(G(z)): 0.0126/0.0095\n",
            "Epoch [10/50] Batch [500/938] Loss_D: 0.3833 Loss_G: 2.7240 D(x): 0.9001 D(G(z)): 0.2054/0.0942\n",
            "Epoch [10/50] Batch [600/938] Loss_D: 0.1032 Loss_G: 5.3013 D(x): 0.9737 D(G(z)): 0.0701/0.0078\n",
            "Epoch [10/50] Batch [700/938] Loss_D: 0.0140 Loss_G: 5.5974 D(x): 0.9940 D(G(z)): 0.0078/0.0066\n",
            "Epoch [10/50] Batch [800/938] Loss_D: 0.0286 Loss_G: 5.3254 D(x): 0.9962 D(G(z)): 0.0242/0.0072\n",
            "Epoch [10/50] Batch [900/938] Loss_D: 0.0071 Loss_G: 7.2725 D(x): 0.9943 D(G(z)): 0.0013/0.0012\n",
            "Epoch [11/50] Batch [0/938] Loss_D: 0.3548 Loss_G: 2.0888 D(x): 0.7855 D(G(z)): 0.0668/0.2065\n",
            "Epoch [11/50] Batch [100/938] Loss_D: 0.4282 Loss_G: 3.5216 D(x): 0.7065 D(G(z)): 0.0082/0.0623\n",
            "Epoch [11/50] Batch [200/938] Loss_D: 0.0524 Loss_G: 4.4633 D(x): 0.9637 D(G(z)): 0.0144/0.0184\n",
            "Epoch [11/50] Batch [300/938] Loss_D: 0.0378 Loss_G: 5.2255 D(x): 0.9933 D(G(z)): 0.0291/0.0082\n",
            "Epoch [11/50] Batch [400/938] Loss_D: 0.0069 Loss_G: 6.2792 D(x): 0.9965 D(G(z)): 0.0033/0.0026\n",
            "Epoch [11/50] Batch [500/938] Loss_D: 0.0160 Loss_G: 6.0755 D(x): 0.9867 D(G(z)): 0.0024/0.0035\n",
            "Epoch [11/50] Batch [600/938] Loss_D: 0.0044 Loss_G: 6.6381 D(x): 0.9979 D(G(z)): 0.0022/0.0019\n",
            "Epoch [11/50] Batch [700/938] Loss_D: 0.0054 Loss_G: 6.3860 D(x): 0.9973 D(G(z)): 0.0026/0.0027\n",
            "Epoch [11/50] Batch [800/938] Loss_D: 0.0036 Loss_G: 6.8531 D(x): 0.9982 D(G(z)): 0.0018/0.0017\n",
            "Epoch [11/50] Batch [900/938] Loss_D: 0.0077 Loss_G: 6.8032 D(x): 0.9941 D(G(z)): 0.0018/0.0016\n",
            "Epoch [12/50] Batch [0/938] Loss_D: 0.0122 Loss_G: 6.0654 D(x): 0.9971 D(G(z)): 0.0091/0.0041\n",
            "Epoch [12/50] Batch [100/938] Loss_D: 0.0086 Loss_G: 6.1620 D(x): 0.9943 D(G(z)): 0.0028/0.0037\n",
            "Epoch [12/50] Batch [200/938] Loss_D: 0.0048 Loss_G: 6.4054 D(x): 0.9987 D(G(z)): 0.0036/0.0024\n",
            "Epoch [12/50] Batch [300/938] Loss_D: 0.0058 Loss_G: 6.6434 D(x): 0.9961 D(G(z)): 0.0019/0.0019\n",
            "Epoch [12/50] Batch [400/938] Loss_D: 0.0034 Loss_G: 7.0648 D(x): 0.9982 D(G(z)): 0.0017/0.0014\n",
            "Epoch [12/50] Batch [500/938] Loss_D: 0.1868 Loss_G: 3.3058 D(x): 0.9343 D(G(z)): 0.0991/0.0544\n",
            "Epoch [12/50] Batch [600/938] Loss_D: 0.1571 Loss_G: 4.4048 D(x): 0.9315 D(G(z)): 0.0717/0.0242\n",
            "Epoch [12/50] Batch [700/938] Loss_D: 0.2622 Loss_G: 4.5278 D(x): 0.9231 D(G(z)): 0.1188/0.0165\n",
            "Epoch [12/50] Batch [800/938] Loss_D: 0.7087 Loss_G: 3.8559 D(x): 0.8567 D(G(z)): 0.3247/0.0434\n",
            "Epoch [12/50] Batch [900/938] Loss_D: 0.1258 Loss_G: 3.8845 D(x): 0.9240 D(G(z)): 0.0334/0.0370\n",
            "Epoch [13/50] Batch [0/938] Loss_D: 0.0709 Loss_G: 5.1757 D(x): 0.9950 D(G(z)): 0.0607/0.0088\n",
            "Epoch [13/50] Batch [100/938] Loss_D: 0.0515 Loss_G: 6.1445 D(x): 0.9959 D(G(z)): 0.0445/0.0031\n",
            "Epoch [13/50] Batch [200/938] Loss_D: 0.1045 Loss_G: 3.8288 D(x): 0.9874 D(G(z)): 0.0836/0.0320\n",
            "Epoch [13/50] Batch [300/938] Loss_D: 0.4346 Loss_G: 4.2176 D(x): 0.9368 D(G(z)): 0.2533/0.0241\n",
            "Epoch [13/50] Batch [400/938] Loss_D: 0.0857 Loss_G: 4.8457 D(x): 0.9593 D(G(z)): 0.0378/0.0176\n",
            "Epoch [13/50] Batch [500/938] Loss_D: 0.0144 Loss_G: 5.7439 D(x): 0.9933 D(G(z)): 0.0075/0.0052\n",
            "Epoch [13/50] Batch [600/938] Loss_D: 0.0112 Loss_G: 5.7952 D(x): 0.9948 D(G(z)): 0.0060/0.0057\n",
            "Epoch [13/50] Batch [700/938] Loss_D: 0.0076 Loss_G: 6.3416 D(x): 0.9948 D(G(z)): 0.0024/0.0025\n",
            "Epoch [13/50] Batch [800/938] Loss_D: 0.0051 Loss_G: 7.3171 D(x): 0.9960 D(G(z)): 0.0011/0.0014\n",
            "Epoch [13/50] Batch [900/938] Loss_D: 0.0054 Loss_G: 6.6807 D(x): 0.9968 D(G(z)): 0.0022/0.0024\n",
            "Epoch [14/50] Batch [0/938] Loss_D: 0.0077 Loss_G: 6.2203 D(x): 0.9953 D(G(z)): 0.0030/0.0031\n",
            "Epoch [14/50] Batch [100/938] Loss_D: 0.0212 Loss_G: 6.0653 D(x): 0.9827 D(G(z)): 0.0035/0.0037\n",
            "Epoch [14/50] Batch [200/938] Loss_D: 0.3987 Loss_G: 2.8450 D(x): 0.7847 D(G(z)): 0.0902/0.0924\n",
            "Epoch [14/50] Batch [300/938] Loss_D: 0.5270 Loss_G: 4.2054 D(x): 0.9336 D(G(z)): 0.2821/0.0281\n",
            "Epoch [14/50] Batch [400/938] Loss_D: 0.2660 Loss_G: 5.3591 D(x): 0.9118 D(G(z)): 0.1172/0.0096\n",
            "Epoch [14/50] Batch [500/938] Loss_D: 0.5224 Loss_G: 3.8868 D(x): 0.9075 D(G(z)): 0.2873/0.0386\n",
            "Epoch [14/50] Batch [600/938] Loss_D: 0.0852 Loss_G: 4.4471 D(x): 0.9709 D(G(z)): 0.0491/0.0192\n",
            "Epoch [14/50] Batch [700/938] Loss_D: 0.0645 Loss_G: 4.2284 D(x): 0.9702 D(G(z)): 0.0319/0.0201\n",
            "Epoch [14/50] Batch [800/938] Loss_D: 0.0178 Loss_G: 5.3137 D(x): 0.9951 D(G(z)): 0.0127/0.0079\n",
            "Epoch [14/50] Batch [900/938] Loss_D: 0.0045 Loss_G: 7.4478 D(x): 0.9969 D(G(z)): 0.0013/0.0013\n",
            "Epoch [15/50] Batch [0/938] Loss_D: 0.0109 Loss_G: 6.4501 D(x): 0.9929 D(G(z)): 0.0037/0.0031\n",
            "Epoch [15/50] Batch [100/938] Loss_D: 0.0076 Loss_G: 6.1245 D(x): 0.9979 D(G(z)): 0.0054/0.0043\n",
            "Epoch [15/50] Batch [200/938] Loss_D: 0.0072 Loss_G: 6.3445 D(x): 0.9946 D(G(z)): 0.0017/0.0028\n",
            "Epoch [15/50] Batch [300/938] Loss_D: 0.0020 Loss_G: 7.8984 D(x): 0.9987 D(G(z)): 0.0007/0.0008\n",
            "Epoch [15/50] Batch [400/938] Loss_D: 0.0024 Loss_G: 7.4449 D(x): 0.9985 D(G(z)): 0.0009/0.0010\n",
            "Epoch [15/50] Batch [500/938] Loss_D: 0.2557 Loss_G: 3.1805 D(x): 0.8282 D(G(z)): 0.0291/0.0760\n",
            "Epoch [15/50] Batch [600/938] Loss_D: 0.1472 Loss_G: 4.1327 D(x): 0.9645 D(G(z)): 0.0963/0.0260\n",
            "Epoch [15/50] Batch [700/938] Loss_D: 3.5243 Loss_G: 14.2583 D(x): 0.9998 D(G(z)): 0.9302/0.0000\n",
            "Epoch [15/50] Batch [800/938] Loss_D: 0.0212 Loss_G: 6.8106 D(x): 0.9830 D(G(z)): 0.0035/0.0024\n",
            "Epoch [15/50] Batch [900/938] Loss_D: 0.0168 Loss_G: 5.6078 D(x): 0.9906 D(G(z)): 0.0071/0.0071\n",
            "Epoch [16/50] Batch [0/938] Loss_D: 0.0143 Loss_G: 5.7306 D(x): 0.9980 D(G(z)): 0.0122/0.0061\n",
            "Epoch [16/50] Batch [100/938] Loss_D: 0.0102 Loss_G: 6.1277 D(x): 0.9940 D(G(z)): 0.0041/0.0037\n",
            "Epoch [16/50] Batch [200/938] Loss_D: 0.0046 Loss_G: 6.6827 D(x): 0.9980 D(G(z)): 0.0026/0.0024\n",
            "Epoch [16/50] Batch [300/938] Loss_D: 0.0053 Loss_G: 6.9973 D(x): 0.9992 D(G(z)): 0.0042/0.0016\n",
            "Epoch [16/50] Batch [400/938] Loss_D: 0.4240 Loss_G: 4.1028 D(x): 0.9406 D(G(z)): 0.2668/0.0313\n",
            "Epoch [16/50] Batch [500/938] Loss_D: 0.0975 Loss_G: 4.2815 D(x): 0.9488 D(G(z)): 0.0408/0.0265\n",
            "Epoch [16/50] Batch [600/938] Loss_D: 0.8932 Loss_G: 1.5033 D(x): 0.6974 D(G(z)): 0.2668/0.3160\n",
            "Epoch [16/50] Batch [700/938] Loss_D: 0.0683 Loss_G: 4.0349 D(x): 0.9667 D(G(z)): 0.0319/0.0280\n",
            "Epoch [16/50] Batch [800/938] Loss_D: 0.0900 Loss_G: 4.4143 D(x): 0.9835 D(G(z)): 0.0654/0.0200\n",
            "Epoch [16/50] Batch [900/938] Loss_D: 0.0603 Loss_G: 4.4339 D(x): 0.9676 D(G(z)): 0.0235/0.0204\n",
            "Epoch [17/50] Batch [0/938] Loss_D: 0.0279 Loss_G: 5.8661 D(x): 0.9769 D(G(z)): 0.0035/0.0053\n",
            "Epoch [17/50] Batch [100/938] Loss_D: 0.0393 Loss_G: 4.7753 D(x): 0.9746 D(G(z)): 0.0126/0.0156\n",
            "Epoch [17/50] Batch [200/938] Loss_D: 0.0165 Loss_G: 5.5573 D(x): 0.9917 D(G(z)): 0.0080/0.0062\n",
            "Epoch [17/50] Batch [300/938] Loss_D: 0.0107 Loss_G: 6.2288 D(x): 0.9963 D(G(z)): 0.0069/0.0044\n",
            "Epoch [17/50] Batch [400/938] Loss_D: 0.0134 Loss_G: 5.3540 D(x): 0.9968 D(G(z)): 0.0100/0.0074\n",
            "Epoch [17/50] Batch [500/938] Loss_D: 0.0053 Loss_G: 6.9212 D(x): 0.9969 D(G(z)): 0.0022/0.0020\n",
            "Epoch [17/50] Batch [600/938] Loss_D: 0.0031 Loss_G: 7.4330 D(x): 0.9984 D(G(z)): 0.0016/0.0014\n",
            "Epoch [17/50] Batch [700/938] Loss_D: 0.0061 Loss_G: 6.7583 D(x): 0.9987 D(G(z)): 0.0048/0.0025\n",
            "Epoch [17/50] Batch [800/938] Loss_D: 0.0023 Loss_G: 7.4233 D(x): 0.9987 D(G(z)): 0.0010/0.0010\n",
            "Epoch [17/50] Batch [900/938] Loss_D: 0.2227 Loss_G: 2.6878 D(x): 0.9014 D(G(z)): 0.0984/0.1020\n",
            "Epoch [18/50] Batch [0/938] Loss_D: 0.7026 Loss_G: 4.9993 D(x): 0.9906 D(G(z)): 0.3979/0.0116\n",
            "Epoch [18/50] Batch [100/938] Loss_D: 0.2445 Loss_G: 3.0597 D(x): 0.8592 D(G(z)): 0.0501/0.0989\n",
            "Epoch [18/50] Batch [200/938] Loss_D: 0.4244 Loss_G: 4.3892 D(x): 0.8244 D(G(z)): 0.1412/0.0348\n",
            "Epoch [18/50] Batch [300/938] Loss_D: 0.0361 Loss_G: 5.4441 D(x): 0.9904 D(G(z)): 0.0248/0.0091\n",
            "Epoch [18/50] Batch [400/938] Loss_D: 0.0096 Loss_G: 6.8216 D(x): 0.9935 D(G(z)): 0.0031/0.0031\n",
            "Epoch [18/50] Batch [500/938] Loss_D: 0.0084 Loss_G: 6.0359 D(x): 0.9971 D(G(z)): 0.0054/0.0039\n",
            "Epoch [18/50] Batch [600/938] Loss_D: 0.0077 Loss_G: 6.2751 D(x): 0.9972 D(G(z)): 0.0049/0.0039\n",
            "Epoch [18/50] Batch [700/938] Loss_D: 0.0052 Loss_G: 7.3240 D(x): 0.9968 D(G(z)): 0.0019/0.0016\n",
            "Epoch [18/50] Batch [800/938] Loss_D: 0.0027 Loss_G: 6.9906 D(x): 0.9991 D(G(z)): 0.0018/0.0017\n",
            "Epoch [18/50] Batch [900/938] Loss_D: 0.2065 Loss_G: 3.3869 D(x): 0.8964 D(G(z)): 0.0690/0.0495\n",
            "Epoch [19/50] Batch [0/938] Loss_D: 0.0976 Loss_G: 3.8522 D(x): 0.9423 D(G(z)): 0.0328/0.0368\n",
            "Epoch [19/50] Batch [100/938] Loss_D: 0.3834 Loss_G: 5.6656 D(x): 0.9307 D(G(z)): 0.2093/0.0071\n",
            "Epoch [19/50] Batch [200/938] Loss_D: 0.0252 Loss_G: 5.9870 D(x): 0.9848 D(G(z)): 0.0093/0.0056\n",
            "Epoch [19/50] Batch [300/938] Loss_D: 0.0053 Loss_G: 6.9215 D(x): 0.9973 D(G(z)): 0.0027/0.0020\n",
            "Epoch [19/50] Batch [400/938] Loss_D: 0.0121 Loss_G: 5.9693 D(x): 0.9936 D(G(z)): 0.0057/0.0055\n",
            "Epoch [19/50] Batch [500/938] Loss_D: 0.0049 Loss_G: 7.9497 D(x): 0.9958 D(G(z)): 0.0006/0.0008\n",
            "Epoch [19/50] Batch [600/938] Loss_D: 0.0067 Loss_G: 6.4320 D(x): 0.9987 D(G(z)): 0.0054/0.0025\n",
            "Epoch [19/50] Batch [700/938] Loss_D: 0.0101 Loss_G: 6.7547 D(x): 0.9928 D(G(z)): 0.0028/0.0020\n",
            "Epoch [19/50] Batch [800/938] Loss_D: 0.3937 Loss_G: 1.9108 D(x): 0.7752 D(G(z)): 0.0928/0.2033\n",
            "Epoch [19/50] Batch [900/938] Loss_D: 0.1449 Loss_G: 4.9671 D(x): 0.8978 D(G(z)): 0.0218/0.0195\n",
            "Epoch [20/50] Batch [0/938] Loss_D: 0.6725 Loss_G: 10.4197 D(x): 0.9991 D(G(z)): 0.4092/0.0001\n",
            "Epoch [20/50] Batch [100/938] Loss_D: 0.0172 Loss_G: 5.6688 D(x): 0.9928 D(G(z)): 0.0096/0.0076\n",
            "Epoch [20/50] Batch [200/938] Loss_D: 0.5531 Loss_G: 1.6562 D(x): 0.6617 D(G(z)): 0.0268/0.2957\n",
            "Epoch [20/50] Batch [300/938] Loss_D: 0.0163 Loss_G: 5.9046 D(x): 0.9937 D(G(z)): 0.0096/0.0061\n",
            "Epoch [20/50] Batch [400/938] Loss_D: 0.0132 Loss_G: 5.4336 D(x): 0.9983 D(G(z)): 0.0113/0.0061\n",
            "Epoch [20/50] Batch [500/938] Loss_D: 0.0040 Loss_G: 7.1090 D(x): 0.9973 D(G(z)): 0.0012/0.0014\n",
            "Epoch [20/50] Batch [600/938] Loss_D: 0.0025 Loss_G: 7.6492 D(x): 0.9983 D(G(z)): 0.0008/0.0008\n",
            "Epoch [20/50] Batch [700/938] Loss_D: 0.0055 Loss_G: 8.3278 D(x): 0.9951 D(G(z)): 0.0006/0.0006\n",
            "Epoch [20/50] Batch [800/938] Loss_D: 0.0035 Loss_G: 6.9153 D(x): 0.9985 D(G(z)): 0.0021/0.0017\n",
            "Epoch [20/50] Batch [900/938] Loss_D: 0.2976 Loss_G: 3.8896 D(x): 0.9601 D(G(z)): 0.2028/0.0272\n",
            "Epoch [21/50] Batch [0/938] Loss_D: 0.6608 Loss_G: 0.9327 D(x): 0.6334 D(G(z)): 0.0377/0.4955\n",
            "Epoch [21/50] Batch [100/938] Loss_D: 0.2540 Loss_G: 2.7762 D(x): 0.8618 D(G(z)): 0.0679/0.1207\n",
            "Epoch [21/50] Batch [200/938] Loss_D: 0.2530 Loss_G: 6.5085 D(x): 0.9940 D(G(z)): 0.1939/0.0026\n",
            "Epoch [21/50] Batch [300/938] Loss_D: 0.0705 Loss_G: 5.1136 D(x): 0.9484 D(G(z)): 0.0124/0.0157\n",
            "Epoch [21/50] Batch [400/938] Loss_D: 0.0358 Loss_G: 5.4673 D(x): 0.9969 D(G(z)): 0.0312/0.0082\n",
            "Epoch [21/50] Batch [500/938] Loss_D: 0.7058 Loss_G: 4.4599 D(x): 0.8809 D(G(z)): 0.3627/0.0183\n",
            "Epoch [21/50] Batch [600/938] Loss_D: 0.0928 Loss_G: 4.2807 D(x): 0.9776 D(G(z)): 0.0654/0.0191\n",
            "Epoch [21/50] Batch [700/938] Loss_D: 0.0315 Loss_G: 5.1266 D(x): 0.9936 D(G(z)): 0.0242/0.0099\n",
            "Epoch [21/50] Batch [800/938] Loss_D: 0.0056 Loss_G: 6.3723 D(x): 0.9977 D(G(z)): 0.0033/0.0036\n",
            "Epoch [21/50] Batch [900/938] Loss_D: 0.0078 Loss_G: 6.5935 D(x): 0.9951 D(G(z)): 0.0029/0.0031\n",
            "Epoch [22/50] Batch [0/938] Loss_D: 0.0124 Loss_G: 6.2664 D(x): 0.9972 D(G(z)): 0.0094/0.0041\n",
            "Epoch [22/50] Batch [100/938] Loss_D: 0.0069 Loss_G: 6.6713 D(x): 0.9978 D(G(z)): 0.0047/0.0031\n",
            "Epoch [22/50] Batch [200/938] Loss_D: 0.0051 Loss_G: 7.1151 D(x): 0.9967 D(G(z)): 0.0017/0.0018\n",
            "Epoch [22/50] Batch [300/938] Loss_D: 0.0030 Loss_G: 7.4487 D(x): 0.9983 D(G(z)): 0.0013/0.0012\n",
            "Epoch [22/50] Batch [400/938] Loss_D: 0.0116 Loss_G: 7.0217 D(x): 0.9892 D(G(z)): 0.0007/0.0024\n",
            "Epoch [22/50] Batch [500/938] Loss_D: 0.0026 Loss_G: 7.0642 D(x): 0.9996 D(G(z)): 0.0022/0.0016\n",
            "Epoch [22/50] Batch [600/938] Loss_D: 0.0018 Loss_G: 7.5503 D(x): 0.9993 D(G(z)): 0.0011/0.0010\n",
            "Epoch [22/50] Batch [700/938] Loss_D: 0.3290 Loss_G: 3.2316 D(x): 0.8953 D(G(z)): 0.1347/0.0667\n",
            "Epoch [22/50] Batch [800/938] Loss_D: 0.2525 Loss_G: 5.8388 D(x): 0.9688 D(G(z)): 0.1667/0.0049\n",
            "Epoch [22/50] Batch [900/938] Loss_D: 0.0762 Loss_G: 4.8650 D(x): 0.9849 D(G(z)): 0.0557/0.0114\n",
            "Epoch [23/50] Batch [0/938] Loss_D: 1.3435 Loss_G: 1.1605 D(x): 0.5119 D(G(z)): 0.1232/0.4642\n",
            "Epoch [23/50] Batch [100/938] Loss_D: 0.1044 Loss_G: 4.9203 D(x): 0.9430 D(G(z)): 0.0283/0.0171\n",
            "Epoch [23/50] Batch [200/938] Loss_D: 0.0284 Loss_G: 5.2271 D(x): 0.9820 D(G(z)): 0.0096/0.0104\n",
            "Epoch [23/50] Batch [300/938] Loss_D: 0.0172 Loss_G: 5.6742 D(x): 0.9909 D(G(z)): 0.0079/0.0069\n",
            "Epoch [23/50] Batch [400/938] Loss_D: 0.0081 Loss_G: 6.2106 D(x): 0.9955 D(G(z)): 0.0036/0.0037\n",
            "Epoch [23/50] Batch [500/938] Loss_D: 0.0072 Loss_G: 7.1094 D(x): 0.9946 D(G(z)): 0.0017/0.0019\n",
            "Epoch [23/50] Batch [600/938] Loss_D: 0.0185 Loss_G: 5.9772 D(x): 0.9886 D(G(z)): 0.0063/0.0056\n",
            "Epoch [23/50] Batch [700/938] Loss_D: 0.0049 Loss_G: 6.7611 D(x): 0.9994 D(G(z)): 0.0043/0.0022\n",
            "Epoch [23/50] Batch [800/938] Loss_D: 0.0036 Loss_G: 6.9830 D(x): 0.9986 D(G(z)): 0.0022/0.0018\n",
            "Epoch [23/50] Batch [900/938] Loss_D: 0.0060 Loss_G: 7.0011 D(x): 0.9994 D(G(z)): 0.0052/0.0024\n",
            "Epoch [24/50] Batch [0/938] Loss_D: 0.0025 Loss_G: 8.1390 D(x): 0.9980 D(G(z)): 0.0005/0.0006\n",
            "Epoch [24/50] Batch [100/938] Loss_D: 0.0023 Loss_G: 7.2531 D(x): 0.9991 D(G(z)): 0.0013/0.0014\n",
            "Epoch [24/50] Batch [200/938] Loss_D: 0.0014 Loss_G: 7.8639 D(x): 0.9993 D(G(z)): 0.0006/0.0006\n",
            "Epoch [24/50] Batch [300/938] Loss_D: 0.3158 Loss_G: 2.8785 D(x): 0.8875 D(G(z)): 0.1277/0.1281\n",
            "Epoch [24/50] Batch [400/938] Loss_D: 0.0774 Loss_G: 4.5192 D(x): 0.9775 D(G(z)): 0.0504/0.0189\n",
            "Epoch [24/50] Batch [500/938] Loss_D: 0.0228 Loss_G: 6.1491 D(x): 0.9826 D(G(z)): 0.0049/0.0044\n",
            "Epoch [24/50] Batch [600/938] Loss_D: 0.4555 Loss_G: 4.8497 D(x): 0.8534 D(G(z)): 0.1492/0.0364\n",
            "Epoch [24/50] Batch [700/938] Loss_D: 0.0176 Loss_G: 5.6776 D(x): 0.9958 D(G(z)): 0.0130/0.0084\n",
            "Epoch [24/50] Batch [800/938] Loss_D: 0.0073 Loss_G: 7.6635 D(x): 0.9944 D(G(z)): 0.0016/0.0010\n",
            "Epoch [24/50] Batch [900/938] Loss_D: 0.0107 Loss_G: 6.4992 D(x): 0.9951 D(G(z)): 0.0055/0.0037\n",
            "Epoch [25/50] Batch [0/938] Loss_D: 0.0075 Loss_G: 8.5751 D(x): 0.9930 D(G(z)): 0.0005/0.0005\n",
            "Epoch [25/50] Batch [100/938] Loss_D: 0.0028 Loss_G: 8.4180 D(x): 0.9979 D(G(z)): 0.0007/0.0007\n",
            "Epoch [25/50] Batch [200/938] Loss_D: 0.0026 Loss_G: 7.3897 D(x): 0.9992 D(G(z)): 0.0018/0.0013\n",
            "Epoch [25/50] Batch [300/938] Loss_D: 0.0052 Loss_G: 7.1964 D(x): 0.9988 D(G(z)): 0.0039/0.0016\n",
            "Epoch [25/50] Batch [400/938] Loss_D: 0.3548 Loss_G: 3.1211 D(x): 0.8043 D(G(z)): 0.0773/0.1285\n",
            "Epoch [25/50] Batch [500/938] Loss_D: 0.1360 Loss_G: 5.0294 D(x): 0.9763 D(G(z)): 0.0908/0.0148\n",
            "Epoch [25/50] Batch [600/938] Loss_D: 0.0754 Loss_G: 4.1301 D(x): 0.9443 D(G(z)): 0.0086/0.0281\n",
            "Epoch [25/50] Batch [700/938] Loss_D: 0.0441 Loss_G: 4.3783 D(x): 0.9940 D(G(z)): 0.0358/0.0244\n",
            "Epoch [25/50] Batch [800/938] Loss_D: 0.0177 Loss_G: 5.9675 D(x): 0.9962 D(G(z)): 0.0136/0.0050\n",
            "Epoch [25/50] Batch [900/938] Loss_D: 0.1707 Loss_G: 4.3647 D(x): 0.9065 D(G(z)): 0.0547/0.0305\n",
            "Epoch [26/50] Batch [0/938] Loss_D: 0.1049 Loss_G: 5.7679 D(x): 0.9892 D(G(z)): 0.0769/0.0057\n",
            "Epoch [26/50] Batch [100/938] Loss_D: 0.0094 Loss_G: 5.7110 D(x): 0.9976 D(G(z)): 0.0070/0.0063\n",
            "Epoch [26/50] Batch [200/938] Loss_D: 0.0140 Loss_G: 5.7145 D(x): 0.9895 D(G(z)): 0.0031/0.0053\n",
            "Epoch [26/50] Batch [300/938] Loss_D: 0.1824 Loss_G: 3.7997 D(x): 0.9117 D(G(z)): 0.0690/0.0418\n",
            "Epoch [26/50] Batch [400/938] Loss_D: 0.0537 Loss_G: 4.5446 D(x): 0.9550 D(G(z)): 0.0057/0.0154\n",
            "Epoch [26/50] Batch [500/938] Loss_D: 0.0230 Loss_G: 5.4902 D(x): 0.9942 D(G(z)): 0.0167/0.0078\n",
            "Epoch [26/50] Batch [600/938] Loss_D: 0.0085 Loss_G: 6.6422 D(x): 0.9945 D(G(z)): 0.0030/0.0029\n",
            "Epoch [26/50] Batch [700/938] Loss_D: 0.0074 Loss_G: 6.6901 D(x): 0.9970 D(G(z)): 0.0044/0.0034\n",
            "Epoch [26/50] Batch [800/938] Loss_D: 0.0125 Loss_G: 6.3010 D(x): 0.9914 D(G(z)): 0.0037/0.0035\n",
            "Epoch [26/50] Batch [900/938] Loss_D: 0.0060 Loss_G: 7.4603 D(x): 0.9963 D(G(z)): 0.0023/0.0020\n",
            "Epoch [27/50] Batch [0/938] Loss_D: 0.0065 Loss_G: 7.9883 D(x): 0.9944 D(G(z)): 0.0009/0.0009\n",
            "Epoch [27/50] Batch [100/938] Loss_D: 0.0063 Loss_G: 6.7955 D(x): 0.9973 D(G(z)): 0.0036/0.0026\n",
            "Epoch [27/50] Batch [200/938] Loss_D: 0.0059 Loss_G: 6.5268 D(x): 0.9989 D(G(z)): 0.0047/0.0030\n",
            "Epoch [27/50] Batch [300/938] Loss_D: 0.0036 Loss_G: 7.2157 D(x): 0.9981 D(G(z)): 0.0017/0.0014\n",
            "Epoch [27/50] Batch [400/938] Loss_D: 0.0062 Loss_G: 6.7466 D(x): 0.9957 D(G(z)): 0.0018/0.0022\n",
            "Epoch [27/50] Batch [500/938] Loss_D: 0.0055 Loss_G: 7.6336 D(x): 0.9952 D(G(z)): 0.0005/0.0010\n",
            "Epoch [27/50] Batch [600/938] Loss_D: 0.0023 Loss_G: 7.5730 D(x): 0.9987 D(G(z)): 0.0010/0.0010\n",
            "Epoch [27/50] Batch [700/938] Loss_D: 0.3075 Loss_G: 3.7241 D(x): 0.7941 D(G(z)): 0.0301/0.0486\n",
            "Epoch [27/50] Batch [800/938] Loss_D: 0.3265 Loss_G: 4.6525 D(x): 0.9455 D(G(z)): 0.1956/0.0170\n",
            "Epoch [27/50] Batch [900/938] Loss_D: 0.2477 Loss_G: 2.8075 D(x): 0.8770 D(G(z)): 0.0812/0.0966\n",
            "Epoch [28/50] Batch [0/938] Loss_D: 0.0954 Loss_G: 5.6423 D(x): 0.9378 D(G(z)): 0.0245/0.0112\n",
            "Epoch [28/50] Batch [100/938] Loss_D: 0.4879 Loss_G: 1.9193 D(x): 0.7357 D(G(z)): 0.0723/0.2728\n",
            "Epoch [28/50] Batch [200/938] Loss_D: 0.0494 Loss_G: 5.4446 D(x): 0.9782 D(G(z)): 0.0252/0.0083\n",
            "Epoch [28/50] Batch [300/938] Loss_D: 0.0123 Loss_G: 5.8803 D(x): 0.9957 D(G(z)): 0.0079/0.0053\n",
            "Epoch [28/50] Batch [400/938] Loss_D: 0.0193 Loss_G: 5.6545 D(x): 0.9988 D(G(z)): 0.0174/0.0071\n",
            "Epoch [28/50] Batch [500/938] Loss_D: 0.0065 Loss_G: 6.4874 D(x): 0.9977 D(G(z)): 0.0041/0.0030\n",
            "Epoch [28/50] Batch [600/938] Loss_D: 0.1781 Loss_G: 3.1175 D(x): 0.8550 D(G(z)): 0.0099/0.0781\n",
            "Epoch [28/50] Batch [700/938] Loss_D: 0.0758 Loss_G: 4.2605 D(x): 0.9689 D(G(z)): 0.0415/0.0231\n",
            "Epoch [28/50] Batch [800/938] Loss_D: 0.0336 Loss_G: 5.4105 D(x): 0.9884 D(G(z)): 0.0212/0.0093\n",
            "Epoch [28/50] Batch [900/938] Loss_D: 0.0062 Loss_G: 6.4674 D(x): 0.9984 D(G(z)): 0.0046/0.0034\n",
            "Epoch [29/50] Batch [0/938] Loss_D: 0.0063 Loss_G: 7.7191 D(x): 0.9957 D(G(z)): 0.0020/0.0019\n",
            "Epoch [29/50] Batch [100/938] Loss_D: 0.0059 Loss_G: 7.1283 D(x): 0.9955 D(G(z)): 0.0014/0.0017\n",
            "Epoch [29/50] Batch [200/938] Loss_D: 0.0051 Loss_G: 7.6880 D(x): 0.9957 D(G(z)): 0.0008/0.0012\n",
            "Epoch [29/50] Batch [300/938] Loss_D: 0.0015 Loss_G: 8.4807 D(x): 0.9989 D(G(z)): 0.0003/0.0004\n",
            "Epoch [29/50] Batch [400/938] Loss_D: 0.0022 Loss_G: 8.8966 D(x): 0.9981 D(G(z)): 0.0004/0.0003\n",
            "Epoch [29/50] Batch [500/938] Loss_D: 0.0029 Loss_G: 7.1499 D(x): 0.9991 D(G(z)): 0.0021/0.0015\n",
            "Epoch [29/50] Batch [600/938] Loss_D: 0.0059 Loss_G: 7.0485 D(x): 0.9991 D(G(z)): 0.0049/0.0017\n",
            "Epoch [29/50] Batch [700/938] Loss_D: 0.0038 Loss_G: 7.1673 D(x): 0.9983 D(G(z)): 0.0021/0.0015\n",
            "Epoch [29/50] Batch [800/938] Loss_D: 0.0013 Loss_G: 8.0394 D(x): 0.9993 D(G(z)): 0.0006/0.0007\n",
            "Epoch [29/50] Batch [900/938] Loss_D: 0.0036 Loss_G: 7.7553 D(x): 0.9972 D(G(z)): 0.0008/0.0009\n",
            "Epoch [30/50] Batch [0/938] Loss_D: 0.0096 Loss_G: 9.5269 D(x): 0.9909 D(G(z)): 0.0002/0.0002\n",
            "Epoch [30/50] Batch [100/938] Loss_D: 0.0039 Loss_G: 7.1330 D(x): 0.9996 D(G(z)): 0.0035/0.0014\n",
            "Epoch [30/50] Batch [200/938] Loss_D: 0.0014 Loss_G: 8.6070 D(x): 0.9995 D(G(z)): 0.0009/0.0006\n",
            "Epoch [30/50] Batch [300/938] Loss_D: 0.0048 Loss_G: 7.7758 D(x): 0.9998 D(G(z)): 0.0045/0.0010\n",
            "Epoch [30/50] Batch [400/938] Loss_D: 0.0503 Loss_G: 5.1335 D(x): 0.9662 D(G(z)): 0.0103/0.0159\n",
            "Epoch [30/50] Batch [500/938] Loss_D: 0.0550 Loss_G: 5.5253 D(x): 0.9835 D(G(z)): 0.0324/0.0093\n",
            "Epoch [30/50] Batch [600/938] Loss_D: 0.0192 Loss_G: 7.1227 D(x): 0.9870 D(G(z)): 0.0059/0.0023\n",
            "Epoch [30/50] Batch [700/938] Loss_D: 0.0080 Loss_G: 6.6724 D(x): 0.9948 D(G(z)): 0.0027/0.0025\n",
            "Epoch [30/50] Batch [800/938] Loss_D: 0.0021 Loss_G: 7.6318 D(x): 0.9990 D(G(z)): 0.0011/0.0011\n",
            "Epoch [30/50] Batch [900/938] Loss_D: 0.0249 Loss_G: 7.8351 D(x): 0.9771 D(G(z)): 0.0002/0.0010\n",
            "Epoch [31/50] Batch [0/938] Loss_D: 0.0012 Loss_G: 8.4678 D(x): 0.9994 D(G(z)): 0.0006/0.0005\n",
            "Epoch [31/50] Batch [100/938] Loss_D: 0.0025 Loss_G: 7.4166 D(x): 0.9998 D(G(z)): 0.0022/0.0012\n",
            "Epoch [31/50] Batch [200/938] Loss_D: 0.0070 Loss_G: 7.6624 D(x): 0.9960 D(G(z)): 0.0029/0.0011\n",
            "Epoch [31/50] Batch [300/938] Loss_D: 0.0026 Loss_G: 7.6143 D(x): 0.9987 D(G(z)): 0.0012/0.0013\n",
            "Epoch [31/50] Batch [400/938] Loss_D: 0.0014 Loss_G: 9.5922 D(x): 0.9988 D(G(z)): 0.0002/0.0002\n",
            "Epoch [31/50] Batch [500/938] Loss_D: 86.7151 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [31/50] Batch [600/938] Loss_D: 0.9108 Loss_G: 2.1140 D(x): 0.5424 D(G(z)): 0.0282/0.2441\n",
            "Epoch [31/50] Batch [700/938] Loss_D: 0.2621 Loss_G: 4.6567 D(x): 0.9394 D(G(z)): 0.1565/0.0151\n",
            "Epoch [31/50] Batch [800/938] Loss_D: 0.1447 Loss_G: 5.4552 D(x): 0.9666 D(G(z)): 0.0893/0.0107\n",
            "Epoch [31/50] Batch [900/938] Loss_D: 0.0602 Loss_G: 5.3889 D(x): 0.9799 D(G(z)): 0.0367/0.0078\n",
            "Epoch [32/50] Batch [0/938] Loss_D: 0.0137 Loss_G: 6.0862 D(x): 0.9901 D(G(z)): 0.0034/0.0063\n",
            "Epoch [32/50] Batch [100/938] Loss_D: 0.4150 Loss_G: 4.0304 D(x): 0.8950 D(G(z)): 0.2287/0.0292\n",
            "Epoch [32/50] Batch [200/938] Loss_D: 0.1181 Loss_G: 4.6490 D(x): 0.9451 D(G(z)): 0.0487/0.0292\n",
            "Epoch [32/50] Batch [300/938] Loss_D: 0.0637 Loss_G: 4.7134 D(x): 0.9569 D(G(z)): 0.0146/0.0215\n",
            "Epoch [32/50] Batch [400/938] Loss_D: 0.0167 Loss_G: 6.0605 D(x): 0.9931 D(G(z)): 0.0095/0.0046\n",
            "Epoch [32/50] Batch [500/938] Loss_D: 0.0187 Loss_G: 6.0577 D(x): 0.9894 D(G(z)): 0.0075/0.0055\n",
            "Epoch [32/50] Batch [600/938] Loss_D: 0.0105 Loss_G: 5.6988 D(x): 0.9987 D(G(z)): 0.0090/0.0062\n",
            "Epoch [32/50] Batch [700/938] Loss_D: 0.0039 Loss_G: 8.7015 D(x): 0.9966 D(G(z)): 0.0005/0.0004\n",
            "Epoch [32/50] Batch [800/938] Loss_D: 0.0038 Loss_G: 6.8900 D(x): 0.9984 D(G(z)): 0.0022/0.0021\n",
            "Epoch [32/50] Batch [900/938] Loss_D: 0.0083 Loss_G: 6.7168 D(x): 0.9945 D(G(z)): 0.0028/0.0024\n",
            "Epoch [33/50] Batch [0/938] Loss_D: 0.0072 Loss_G: 6.6994 D(x): 0.9981 D(G(z)): 0.0052/0.0028\n",
            "Epoch [33/50] Batch [100/938] Loss_D: 0.0038 Loss_G: 7.2220 D(x): 0.9992 D(G(z)): 0.0030/0.0017\n",
            "Epoch [33/50] Batch [200/938] Loss_D: 0.2900 Loss_G: 3.5374 D(x): 0.9255 D(G(z)): 0.1612/0.0566\n",
            "Epoch [33/50] Batch [300/938] Loss_D: 0.2208 Loss_G: 4.0415 D(x): 0.9540 D(G(z)): 0.1457/0.0251\n",
            "Epoch [33/50] Batch [400/938] Loss_D: 0.0511 Loss_G: 5.7443 D(x): 0.9853 D(G(z)): 0.0342/0.0061\n",
            "Epoch [33/50] Batch [500/938] Loss_D: 0.0182 Loss_G: 5.9377 D(x): 0.9907 D(G(z)): 0.0084/0.0058\n",
            "Epoch [33/50] Batch [600/938] Loss_D: 0.0070 Loss_G: 6.6994 D(x): 0.9970 D(G(z)): 0.0039/0.0028\n",
            "Epoch [33/50] Batch [700/938] Loss_D: 0.0052 Loss_G: 7.3692 D(x): 0.9962 D(G(z)): 0.0014/0.0016\n",
            "Epoch [33/50] Batch [800/938] Loss_D: 0.0088 Loss_G: 6.4514 D(x): 0.9981 D(G(z)): 0.0069/0.0027\n",
            "Epoch [33/50] Batch [900/938] Loss_D: 0.0105 Loss_G: 6.6402 D(x): 0.9942 D(G(z)): 0.0046/0.0041\n",
            "Epoch [34/50] Batch [0/938] Loss_D: 0.0032 Loss_G: 7.2053 D(x): 0.9989 D(G(z)): 0.0021/0.0018\n",
            "Epoch [34/50] Batch [100/938] Loss_D: 0.0055 Loss_G: 7.2255 D(x): 0.9959 D(G(z)): 0.0014/0.0017\n",
            "Epoch [34/50] Batch [200/938] Loss_D: 0.0032 Loss_G: 7.2283 D(x): 0.9990 D(G(z)): 0.0022/0.0017\n",
            "Epoch [34/50] Batch [300/938] Loss_D: 0.0061 Loss_G: 6.8578 D(x): 0.9980 D(G(z)): 0.0041/0.0028\n",
            "Epoch [34/50] Batch [400/938] Loss_D: 0.0017 Loss_G: 8.2846 D(x): 0.9991 D(G(z)): 0.0008/0.0007\n",
            "Epoch [34/50] Batch [500/938] Loss_D: 0.0027 Loss_G: 8.3433 D(x): 0.9976 D(G(z)): 0.0003/0.0005\n",
            "Epoch [34/50] Batch [600/938] Loss_D: 0.0021 Loss_G: 8.1043 D(x): 0.9991 D(G(z)): 0.0011/0.0007\n",
            "Epoch [34/50] Batch [700/938] Loss_D: 0.0744 Loss_G: 6.0975 D(x): 0.9508 D(G(z)): 0.0002/0.0080\n",
            "Epoch [34/50] Batch [800/938] Loss_D: 0.3373 Loss_G: 3.7904 D(x): 0.9583 D(G(z)): 0.2271/0.0491\n",
            "Epoch [34/50] Batch [900/938] Loss_D: 0.5018 Loss_G: 5.4419 D(x): 0.9773 D(G(z)): 0.2979/0.0121\n",
            "Epoch [35/50] Batch [0/938] Loss_D: 0.0920 Loss_G: 4.9630 D(x): 0.9590 D(G(z)): 0.0414/0.0185\n",
            "Epoch [35/50] Batch [100/938] Loss_D: 0.0612 Loss_G: 4.7391 D(x): 0.9771 D(G(z)): 0.0336/0.0218\n",
            "Epoch [35/50] Batch [200/938] Loss_D: 0.1191 Loss_G: 5.2127 D(x): 0.9482 D(G(z)): 0.0473/0.0172\n",
            "Epoch [35/50] Batch [300/938] Loss_D: 0.0327 Loss_G: 4.8757 D(x): 0.9792 D(G(z)): 0.0111/0.0144\n",
            "Epoch [35/50] Batch [400/938] Loss_D: 0.0133 Loss_G: 5.1424 D(x): 0.9958 D(G(z)): 0.0090/0.0091\n",
            "Epoch [35/50] Batch [500/938] Loss_D: 0.0257 Loss_G: 5.2545 D(x): 0.9944 D(G(z)): 0.0196/0.0084\n",
            "Epoch [35/50] Batch [600/938] Loss_D: 0.0136 Loss_G: 5.8640 D(x): 0.9947 D(G(z)): 0.0080/0.0060\n",
            "Epoch [35/50] Batch [700/938] Loss_D: 0.0046 Loss_G: 6.6120 D(x): 0.9984 D(G(z)): 0.0030/0.0026\n",
            "Epoch [35/50] Batch [800/938] Loss_D: 0.0039 Loss_G: 6.8188 D(x): 0.9981 D(G(z)): 0.0020/0.0019\n",
            "Epoch [35/50] Batch [900/938] Loss_D: 0.0046 Loss_G: 6.1906 D(x): 0.9984 D(G(z)): 0.0030/0.0031\n",
            "Epoch [36/50] Batch [0/938] Loss_D: 0.0055 Loss_G: 6.9999 D(x): 0.9956 D(G(z)): 0.0011/0.0018\n",
            "Epoch [36/50] Batch [100/938] Loss_D: 0.0064 Loss_G: 6.6254 D(x): 0.9983 D(G(z)): 0.0046/0.0030\n",
            "Epoch [36/50] Batch [200/938] Loss_D: 0.0042 Loss_G: 6.9893 D(x): 0.9995 D(G(z)): 0.0036/0.0022\n",
            "Epoch [36/50] Batch [300/938] Loss_D: 0.0012 Loss_G: 7.7665 D(x): 0.9994 D(G(z)): 0.0006/0.0009\n",
            "Epoch [36/50] Batch [400/938] Loss_D: 0.0043 Loss_G: 7.3302 D(x): 0.9966 D(G(z)): 0.0009/0.0012\n",
            "Epoch [36/50] Batch [500/938] Loss_D: 0.0023 Loss_G: 7.3014 D(x): 0.9995 D(G(z)): 0.0018/0.0013\n",
            "Epoch [36/50] Batch [600/938] Loss_D: 0.0054 Loss_G: 6.9039 D(x): 0.9994 D(G(z)): 0.0047/0.0019\n",
            "Epoch [36/50] Batch [700/938] Loss_D: 0.0054 Loss_G: 8.5951 D(x): 0.9948 D(G(z)): 0.0002/0.0004\n",
            "Epoch [36/50] Batch [800/938] Loss_D: 0.0022 Loss_G: 7.1360 D(x): 0.9995 D(G(z)): 0.0017/0.0015\n",
            "Epoch [36/50] Batch [900/938] Loss_D: 0.0018 Loss_G: 7.9188 D(x): 0.9997 D(G(z)): 0.0015/0.0009\n",
            "Epoch [37/50] Batch [0/938] Loss_D: 0.0010 Loss_G: 9.8405 D(x): 0.9992 D(G(z)): 0.0002/0.0002\n",
            "Epoch [37/50] Batch [100/938] Loss_D: 0.0011 Loss_G: 8.3772 D(x): 0.9995 D(G(z)): 0.0006/0.0005\n",
            "Epoch [37/50] Batch [200/938] Loss_D: 0.0031 Loss_G: 8.4988 D(x): 0.9973 D(G(z)): 0.0004/0.0006\n",
            "Epoch [37/50] Batch [300/938] Loss_D: 0.0018 Loss_G: 9.5428 D(x): 0.9983 D(G(z)): 0.0001/0.0002\n",
            "Epoch [37/50] Batch [400/938] Loss_D: 0.0015 Loss_G: 7.9721 D(x): 0.9997 D(G(z)): 0.0012/0.0008\n",
            "Epoch [37/50] Batch [500/938] Loss_D: 0.0008 Loss_G: 7.8464 D(x): 0.9996 D(G(z)): 0.0005/0.0007\n",
            "Epoch [37/50] Batch [600/938] Loss_D: 0.4445 Loss_G: 3.3963 D(x): 0.8742 D(G(z)): 0.2056/0.0819\n",
            "Epoch [37/50] Batch [700/938] Loss_D: 0.2247 Loss_G: 3.1280 D(x): 0.9061 D(G(z)): 0.0979/0.0729\n",
            "Epoch [37/50] Batch [800/938] Loss_D: 0.2184 Loss_G: 6.2098 D(x): 0.8850 D(G(z)): 0.0393/0.0087\n",
            "Epoch [37/50] Batch [900/938] Loss_D: 0.1163 Loss_G: 4.6842 D(x): 0.9743 D(G(z)): 0.0744/0.0249\n",
            "Epoch [38/50] Batch [0/938] Loss_D: 0.4528 Loss_G: 6.0133 D(x): 0.7413 D(G(z)): 0.0023/0.0117\n",
            "Epoch [38/50] Batch [100/938] Loss_D: 0.0423 Loss_G: 6.1761 D(x): 0.9717 D(G(z)): 0.0090/0.0064\n",
            "Epoch [38/50] Batch [200/938] Loss_D: 0.5395 Loss_G: 2.7136 D(x): 0.7918 D(G(z)): 0.1498/0.1512\n",
            "Epoch [38/50] Batch [300/938] Loss_D: 0.0336 Loss_G: 5.0157 D(x): 0.9932 D(G(z)): 0.0239/0.0131\n",
            "Epoch [38/50] Batch [400/938] Loss_D: 0.0267 Loss_G: 5.4102 D(x): 0.9831 D(G(z)): 0.0089/0.0090\n",
            "Epoch [38/50] Batch [500/938] Loss_D: 0.0229 Loss_G: 5.5336 D(x): 0.9885 D(G(z)): 0.0111/0.0073\n",
            "Epoch [38/50] Batch [600/938] Loss_D: 0.0125 Loss_G: 6.5425 D(x): 0.9901 D(G(z)): 0.0024/0.0027\n",
            "Epoch [38/50] Batch [700/938] Loss_D: 0.0135 Loss_G: 6.0641 D(x): 0.9959 D(G(z)): 0.0092/0.0048\n",
            "Epoch [38/50] Batch [800/938] Loss_D: 0.0054 Loss_G: 6.8814 D(x): 0.9979 D(G(z)): 0.0033/0.0023\n",
            "Epoch [38/50] Batch [900/938] Loss_D: 0.0055 Loss_G: 8.7396 D(x): 0.9950 D(G(z)): 0.0004/0.0006\n",
            "Epoch [39/50] Batch [0/938] Loss_D: 0.0056 Loss_G: 6.6073 D(x): 0.9981 D(G(z)): 0.0037/0.0025\n",
            "Epoch [39/50] Batch [100/938] Loss_D: 0.0073 Loss_G: 7.2761 D(x): 0.9951 D(G(z)): 0.0024/0.0020\n",
            "Epoch [39/50] Batch [200/938] Loss_D: 0.0140 Loss_G: 6.3809 D(x): 0.9924 D(G(z)): 0.0062/0.0042\n",
            "Epoch [39/50] Batch [300/938] Loss_D: 0.2195 Loss_G: 3.4024 D(x): 0.8765 D(G(z)): 0.0446/0.0661\n",
            "Epoch [39/50] Batch [400/938] Loss_D: 0.0980 Loss_G: 4.9666 D(x): 0.9645 D(G(z)): 0.0567/0.0132\n",
            "Epoch [39/50] Batch [500/938] Loss_D: 0.0092 Loss_G: 6.3504 D(x): 0.9973 D(G(z)): 0.0064/0.0038\n",
            "Epoch [39/50] Batch [600/938] Loss_D: 0.0133 Loss_G: 7.9649 D(x): 0.9903 D(G(z)): 0.0032/0.0013\n",
            "Epoch [39/50] Batch [700/938] Loss_D: 0.0359 Loss_G: 7.4645 D(x): 0.9976 D(G(z)): 0.0283/0.0013\n",
            "Epoch [39/50] Batch [800/938] Loss_D: 0.0025 Loss_G: 8.2630 D(x): 0.9986 D(G(z)): 0.0011/0.0013\n",
            "Epoch [39/50] Batch [900/938] Loss_D: 0.0034 Loss_G: 7.7037 D(x): 0.9979 D(G(z)): 0.0012/0.0013\n",
            "Epoch [40/50] Batch [0/938] Loss_D: 0.0047 Loss_G: 6.7878 D(x): 0.9988 D(G(z)): 0.0034/0.0024\n",
            "Epoch [40/50] Batch [100/938] Loss_D: 0.0048 Loss_G: 8.7537 D(x): 0.9954 D(G(z)): 0.0002/0.0004\n",
            "Epoch [40/50] Batch [200/938] Loss_D: 0.0039 Loss_G: 8.0699 D(x): 0.9969 D(G(z)): 0.0008/0.0007\n",
            "Epoch [40/50] Batch [300/938] Loss_D: 0.0019 Loss_G: 7.4389 D(x): 0.9996 D(G(z)): 0.0015/0.0012\n",
            "Epoch [40/50] Batch [400/938] Loss_D: 0.0076 Loss_G: 7.2519 D(x): 0.9935 D(G(z)): 0.0009/0.0018\n",
            "Epoch [40/50] Batch [500/938] Loss_D: 0.0022 Loss_G: 8.8901 D(x): 0.9980 D(G(z)): 0.0003/0.0003\n",
            "Epoch [40/50] Batch [600/938] Loss_D: 0.3051 Loss_G: 4.6407 D(x): 0.8050 D(G(z)): 0.0385/0.0388\n",
            "Epoch [40/50] Batch [700/938] Loss_D: 0.4545 Loss_G: 3.8519 D(x): 0.9138 D(G(z)): 0.2211/0.0394\n",
            "Epoch [40/50] Batch [800/938] Loss_D: 0.2609 Loss_G: 4.1774 D(x): 0.8398 D(G(z)): 0.0447/0.0448\n",
            "Epoch [40/50] Batch [900/938] Loss_D: 0.3156 Loss_G: 4.3298 D(x): 0.9621 D(G(z)): 0.1997/0.0269\n",
            "Epoch [41/50] Batch [0/938] Loss_D: 0.1481 Loss_G: 4.5195 D(x): 0.9196 D(G(z)): 0.0435/0.0314\n",
            "Epoch [41/50] Batch [100/938] Loss_D: 0.4224 Loss_G: 2.9707 D(x): 0.7500 D(G(z)): 0.0523/0.1120\n",
            "Epoch [41/50] Batch [200/938] Loss_D: 0.0187 Loss_G: 6.1053 D(x): 0.9880 D(G(z)): 0.0064/0.0056\n",
            "Epoch [41/50] Batch [300/938] Loss_D: 0.0175 Loss_G: 5.8566 D(x): 0.9975 D(G(z)): 0.0145/0.0060\n",
            "Epoch [41/50] Batch [400/938] Loss_D: 0.0092 Loss_G: 6.2474 D(x): 0.9983 D(G(z)): 0.0074/0.0041\n",
            "Epoch [41/50] Batch [500/938] Loss_D: 0.0038 Loss_G: 7.2179 D(x): 0.9987 D(G(z)): 0.0025/0.0021\n",
            "Epoch [41/50] Batch [600/938] Loss_D: 0.0066 Loss_G: 7.3397 D(x): 0.9956 D(G(z)): 0.0021/0.0022\n",
            "Epoch [41/50] Batch [700/938] Loss_D: 0.0024 Loss_G: 8.0811 D(x): 0.9989 D(G(z)): 0.0013/0.0011\n",
            "Epoch [41/50] Batch [800/938] Loss_D: 0.0028 Loss_G: 7.4608 D(x): 0.9991 D(G(z)): 0.0019/0.0014\n",
            "Epoch [41/50] Batch [900/938] Loss_D: 0.0120 Loss_G: 6.5349 D(x): 0.9973 D(G(z)): 0.0092/0.0032\n",
            "Epoch [42/50] Batch [0/938] Loss_D: 0.0081 Loss_G: 8.0018 D(x): 0.9925 D(G(z)): 0.0006/0.0010\n",
            "Epoch [42/50] Batch [100/938] Loss_D: 0.0016 Loss_G: 7.9237 D(x): 0.9992 D(G(z)): 0.0009/0.0007\n",
            "Epoch [42/50] Batch [200/938] Loss_D: 0.0014 Loss_G: 8.8657 D(x): 0.9991 D(G(z)): 0.0004/0.0004\n",
            "Epoch [42/50] Batch [300/938] Loss_D: 0.0032 Loss_G: 8.0334 D(x): 0.9976 D(G(z)): 0.0008/0.0006\n",
            "Epoch [42/50] Batch [400/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [42/50] Batch [500/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [42/50] Batch [600/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [42/50] Batch [700/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [42/50] Batch [800/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [42/50] Batch [900/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [43/50] Batch [0/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [43/50] Batch [100/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [43/50] Batch [200/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [43/50] Batch [300/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [43/50] Batch [400/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [43/50] Batch [500/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [43/50] Batch [600/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [43/50] Batch [700/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [43/50] Batch [800/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [43/50] Batch [900/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [44/50] Batch [0/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [44/50] Batch [100/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [44/50] Batch [200/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [44/50] Batch [300/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [44/50] Batch [400/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [44/50] Batch [500/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [44/50] Batch [600/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [44/50] Batch [700/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [44/50] Batch [800/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [44/50] Batch [900/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [45/50] Batch [0/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [45/50] Batch [100/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [45/50] Batch [200/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [45/50] Batch [300/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [45/50] Batch [400/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [45/50] Batch [500/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [45/50] Batch [600/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [45/50] Batch [700/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [45/50] Batch [800/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [45/50] Batch [900/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [46/50] Batch [0/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [46/50] Batch [100/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [46/50] Batch [200/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [46/50] Batch [300/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [46/50] Batch [400/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [46/50] Batch [500/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [46/50] Batch [600/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [46/50] Batch [700/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [46/50] Batch [800/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [46/50] Batch [900/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [47/50] Batch [0/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [47/50] Batch [100/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [47/50] Batch [200/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [47/50] Batch [300/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [47/50] Batch [400/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [47/50] Batch [500/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [47/50] Batch [600/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [47/50] Batch [700/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [47/50] Batch [800/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [47/50] Batch [900/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [48/50] Batch [0/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [48/50] Batch [100/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [48/50] Batch [200/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [48/50] Batch [300/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [48/50] Batch [400/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [48/50] Batch [500/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [48/50] Batch [600/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [48/50] Batch [700/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [48/50] Batch [800/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [48/50] Batch [900/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [49/50] Batch [0/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [49/50] Batch [100/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [49/50] Batch [200/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [49/50] Batch [300/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [49/50] Batch [400/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [49/50] Batch [500/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [49/50] Batch [600/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [49/50] Batch [700/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [49/50] Batch [800/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [49/50] Batch [900/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [50/50] Batch [0/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [50/50] Batch [100/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [50/50] Batch [200/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [50/50] Batch [300/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [50/50] Batch [400/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [50/50] Batch [500/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [50/50] Batch [600/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [50/50] Batch [700/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [50/50] Batch [800/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Epoch [50/50] Batch [900/938] Loss_D: 100.0000 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000/1.0000\n",
            "Training Finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set the generator to evaluation mode\n",
        "netG.eval()\n",
        "\n",
        "# Define the label for which you want to generate MNIST-M images (e.g., 5)\n",
        "test_label = 8\n",
        "num_samples = 10  # Number of images to generate\n",
        "\n",
        "# Generate random noise\n",
        "noise = torch.randn(num_samples, opt['nz'], 1, 1, device=device)\n",
        "\n",
        "# Create one-hot encoding of the test label\n",
        "test_label_tensor = torch.tensor([test_label] * num_samples, device=device)\n",
        "test_label_one_hot = torch.nn.functional.one_hot(test_label_tensor, opt['num_classes']).type(torch.float).to(device)\n",
        "test_label_one_hot_gen = test_label_one_hot.unsqueeze(2).unsqueeze(3)\n",
        "\n",
        "# Concatenate the noise and the label\n",
        "noise_with_labels = torch.cat((noise, test_label_one_hot_gen), 1)\n",
        "\n",
        "# Generate fake MNIST-M images\n",
        "with torch.no_grad():  # No need to track gradients during inference\n",
        "    fake_images = netG(noise_with_labels).cpu()\n",
        "\n",
        "# Display generated images\n",
        "fig, axes = plt.subplots(1, num_samples, figsize=(num_samples, 2))\n",
        "for i in range(num_samples):\n",
        "    # Assuming MNIST-M images are RGB, we use transpose to convert the generated tensor to (H, W, C)\n",
        "    axes[i].imshow(fake_images[i].permute(1, 2, 0))  # permute to (height, width, channels) for RGB\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "pU6AGAdPPUUf",
        "outputId": "972487b6-b730-4fda-f5e8-a9686c4d5c8b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x200 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}